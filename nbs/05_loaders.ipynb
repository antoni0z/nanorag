{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8404bfc-069b-49b4-b140-49c850418b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc954c-662d-4ac0-96f2-84bcb70a8950",
   "metadata": {},
   "source": [
    "# Loaders\n",
    "\n",
    "> A module for importing data and converting it to a processable output for the most typical file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef0a0e-b785-4395-8f52-8c42e9998aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import random\n",
    "from typing import List\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48c660-a2f3-4984-b1ec-43f15d7d2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from nanorag.store import *\n",
    "from nanorag.base import *\n",
    "from nanorag.context import *\n",
    "from nanorag.llm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945c117-c52c-4816-822b-235d32a89caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f679303-536a-42ac-a5a0-e62315cbaccc",
   "metadata": {},
   "source": [
    "#|hide\n",
    "We have a set of PDF files we previously downloaded to test out and create a PDF loader. We will try to create a loader that extract both images and text in an structured way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099616f-6a84-4c8c-ac35-09f4df9cbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#For simplicity lets start with accepting a List. \n",
    "class PDFLoader:\n",
    "    \"\"\"Accepts a dir or single path and converts its contents into documents that can be later used for storage and retrieval\"\"\"\n",
    "    def __init__(self, path_dir: str):\n",
    "        self.path_dir = Path(path_dir)\n",
    "        if self.path_dir.is_dir():\n",
    "            self.paths = [path for path in self.path_dir.iterdir() if path]\n",
    "        else:\n",
    "            self.paths = [self.path_dir]\n",
    "        self.path = None\n",
    "        \n",
    "    def pdf_validator(self, path):\n",
    "        \"\"\"Tries to read the pdf and returns a Bool value with the result\"\"\"\n",
    "        try:\n",
    "            reader = PdfReader(path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "    def load_random_pdf(self):\n",
    "        \"\"\"Load a random pdf from the dataset. It loads pdfs until a valid one is found\"\"\"\n",
    "        valid_pdf_found = False\n",
    "        while not valid_pdf_found:  # Continue until a valid PDF is found\n",
    "            pdf_path = random.choice(self.paths)\n",
    "            is_valid = self.pdf_validator(pdf_path)\n",
    "            if is_valid:\n",
    "                reader = PdfReader(pdf_path)\n",
    "                valid_pdf_found = True\n",
    "                self.path = pdf_path\n",
    "                return reader\n",
    "            else:\n",
    "                pdf_path.unlink()\n",
    "                self.paths.remove(pdf_path)  # Remove the invalid path from the list\n",
    "        \n",
    "        if not valid_pdf_found:\n",
    "            return None\n",
    "    def load_pdf(self, path):\n",
    "        reader = PdfReader(path)\n",
    "        self.path = path\n",
    "        return reader\n",
    "    \n",
    "    def get_documents(self, path = None):\n",
    "        \"\"\"Get a List of Text Documents from a pdf Path.\"\"\"\n",
    "        documents = []\n",
    "        #Extracting text and storing it in documents\n",
    "        if path == None:\n",
    "            reader = self.load_random_pdf()\n",
    "        else:\n",
    "            reader = self.load_pdf(path)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            params = {\"metadata\": {**{\"page\": i + 1}, **reader.metadata}, \"text\": page.extract_text()}\n",
    "            if i == 0:\n",
    "                title = reader.metadata.get('title', None)\n",
    "                if title is None:\n",
    "                    title = params['text'].split('\\n')[0]        \n",
    "            if title is not None:\n",
    "                params[\"name\"] = title\n",
    "            doc = Document(**params)\n",
    "            documents.append(doc)\n",
    "        return documents\n",
    "    def get_images(self, path = None):\n",
    "        #Can add some metadata like what page and location was found on. \n",
    "        #Create Image Node with that kind of info. \n",
    "        if path == None:\n",
    "            reader = self.load_random_pdf()\n",
    "        else:\n",
    "            reader = self.load_pdf(path)\n",
    "        images = []\n",
    "        for count, page in enumerate(reader.pages):\n",
    "            for image_file_object in page.images:\n",
    "                image = Image.open(BytesIO(image_file_object.data))\n",
    "                images.append(image)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068f724-47ea-4192-9986-d340776d8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DocumentBridge:\n",
    "    \"\"\"Class for connecting a list of documents into its corresponding Nodes and relationships\"\"\"\n",
    "    def __init__(self, documents: List, context: ModelContext):\n",
    "        if isinstance(documents, List):\n",
    "            self.documents = documents\n",
    "        else:\n",
    "            raise \"You have to include a List of documents\"\n",
    "        self.context = context\n",
    "    def nodes(self, chunk_size = 1024) -> List[TextNode]:\n",
    "        \"\"\"Brige a series of Documents into nodes linked by the end and start of the prev and next document. Great for linking together complex docs with structure\n",
    "        such as pages or other info extracted first on a Document basis.\"\"\"\n",
    "        doc_nodes_list = [doc.create_nodes_from_doc(self.context, chunk_size = chunk_size) for doc in self.documents]\n",
    "        for i, node_list in enumerate(doc_nodes_list):\n",
    "            if i == 0:\n",
    "                node_list[-1].next_node = doc_nodes_list[i + 1][0].id\n",
    "            else:\n",
    "                if i < len(doc_nodes_list) - 1:\n",
    "                    node_list[-1].next_node = doc_nodes_list[i + 1][0].id\n",
    "                node_list[0].prev_node = doc_nodes_list[i - 1][-1].id\n",
    "        nodes = [node for node_list in doc_nodes_list for node in node_list]\n",
    "        return nodes\n",
    "        \n",
    "    def join(self) -> Document:\n",
    "        \"\"\"Bridges a series of Documents into a single document. Great for storing sub-documents into a single one. Keeps some metadata of the documents into one. \"\"\"\n",
    "        #Store metadata about length, pages etc. For the later processing to be better. Maybe metadata about where each page started and ended in terms of characters could be good. \n",
    "        #see tradeoffs between this and diff docs pointing to a single reference. \n",
    "        #In reality in the conversion to nodes all the info is kept. We can post-process there. \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448a0f8-8817-4b41-9d1e-13e7f57d1c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eb869fdcb5486ca921f9b467c01139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map = \"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "llm = LLM(model = model, tokenizer = tokenizer)\n",
    "context = ModelContext(llm = llm, embedding = embedding_model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b246bb-f801-412d-96d3-54d06dcf39b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id = 6a7d5f94-268c-44da-a8a1-9656efa76c38,text = Self-training and Pre-training are Complementary\n",
       " for Speech Recognition\n",
       " Qiantong Xu\u0003Alexei Baevski\u0003Tatiana Likhomanenko Paden Tomasello\n",
       " Alexis Conneau Ronan Collobert Gabriel Synnaeve Michael Auli\n",
       " Facebook AI Research\n",
       " Abstract\n",
       " Self-training and unsupervised pre-training have emerged as effective approaches\n",
       " to improve speech recognition systems using unlabeled data. However, it is not\n",
       " clear whether they learn similar patterns or if they can be effectively combined.\n",
       " In this paper, we show that pseudo-labeling and pre-training with wav2vec 2.0\n",
       " are complementary in a variety of labeled data setups. Using just 10 minutes of\n",
       " labeled data from Libri-light as well as 53k hours of unlabeled data from LibriV ox\n",
       " achieves WERs of 3.0%/5.2% on the clean and other test sets of Librispeech –\n",
       " rivaling the best published systems trained on 960 hours of labeled data only a year\n",
       " ago. Training on all labeled data of Librispeech achieves WERs of 1.5%/3.1%.\n",
       " 1 Introduction\n",
       " Speech recognition models trained on labeled speech data has progressed substantially in the recent\n",
       " past [ 1,2,3,4]. A drawback of these models is that they require a lot of labeled data to perform well\n",
       " which is usually only available for English and a few other languages. Therefore, purely supervised\n",
       " training is impractical for the vast majority of the 7,000 languages spoken around the world [ 5] which\n",
       " is why there has been a lot of interest in how to better use unlabeled speech data [6, 7, 8].\n",
       " This includes classical self-training [ 9,10,11] which demonstrated strong results [ 12,13,2,14,15]\n",
       " by pseudo-labeling unannotated audio data and then retraining the ﬁnal system with the additional\n",
       " labeled data. Another line of work is pre-training representations on unlabeled speech followed by\n",
       " ﬁne-tuning on labeled data [16, 17, 18, 19, 20, 21, 22, 23, 24].\n",
       " In this paper we combine self-training and unsupervised pre-training which are different approaches\n",
       " to leveraging unlabeled data. Both achieved excellent results on competitive benchmarks and the\n",
       " central question we explore is whether the two methods are complementary to each other. Speciﬁcally,\n",
       " we build on the recently introduced wav2vec 2.0 model [ 24] and the self-training approach of Kahn\n",
       " et al. (2020; [ 13]) and Xu et al. (2020; [ 14]). We explore training models on the pseudo-labeled data\n",
       " from scratch or by ﬁne-tuning the pre-trained model. To better understand how complementary the\n",
       " two methods are, we use the same unlabeled data for both.\n",
       " Experiments on the,metadata = {'page': 1, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = None, next_node = fc0894f6-1e31-415d-b218-545804e50970, parent_node = None, child_node = []),\n",
       " TextNode(id = fc0894f6-1e31-415d-b218-545804e50970,text =  full Librispeech corpus as well as the low-resource labeled data setups of Libri-\n",
       " light show that self-training and unsupervised pre-training are indeed complementary, a ﬁnding that\n",
       " is inline with recent work in natural language understanding [ 25]. In a very low resource setup with\n",
       " just 10 minutes of labeled data and LibriV ox as unlabeled data, the combination of wav2vec 2.0\n",
       " and self-training achieves a WER of 3.0%/5.2% on the clean and other test sets of Librispeech, a\n",
       " \u0003Equal contribution.\n",
       " Preprint. Under review.arXiv:2010.11430v1  [cs.LG]  22 Oct 2020,metadata = {'page': 1, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 6a7d5f94-268c-44da-a8a1-9656efa76c38, next_node = 560ab392-b850-4494-8762-121c22bbca02, parent_node = None, child_node = []),\n",
       " TextNode(id = 560ab392-b850-4494-8762-121c22bbca02,text = relative WER reduction of 25% and 40% over recent work in pre-training alone [ 24]. Using just the\n",
       " acoustic model without a language model achieves WER 3.7%/6.5% - supporting the hypothesis that\n",
       " self-training distills the language model used for pseudo-labeling into the ﬁnal model. When all 960\n",
       " hours of labeled training data are used we achieve 1.5%/3.1% WER on Librispeech.\n",
       " 2 Background\n",
       " 2.1 Unsupervised Pre-training Model\n",
       " We experiment with the recently introduced wav2vec 2.0 model of Baevski et al. (2020; [ 24]).\n",
       " This model contains a convolutional feature encoder f:X 7!Z to map raw audio Xto latent\n",
       " speech representations z1; : : : ;zTwhich are input to a Transformer g:Z7!C to output context\n",
       " representations c1; : : : ;cT[26,18,27]. Each ztrepresents about 25ms of audio strided by 20ms and\n",
       " the Transformer architecture follows BERT [ 28,26]. During training, feature encoder representations\n",
       " are discretized to q1; : : : ;qTwith a quantization module Z7!Q to represent the targets in the\n",
       " objective. The quantization module uses a Gumbel softmax to choose entries from G= 2codebooks\n",
       " withV= 320 entries each and the chosen entries are concatenated to obtain q[29, 30, 18].\n",
       " The model is trained by solving a contrastive task over masked feature encoder outputs. At training\n",
       " time, spans of ten time steps with random starting indices are masked. The objective requires\n",
       " identifying the true quantized latent qtfor a masked time-step within a set of K= 100 distractors\n",
       " Qtsampled from other masked time steps: \u0000logexp(sim(ct;qt))P\n",
       " ~ q\u0018Qtexp(sim(ct;~ q))where ctis the output of the\n",
       " Transformer, and sim(a;b)denotes cosine similarity. The objective is augmented by a codebook\n",
       " diversity penalty to encourage the model to use all codebook entries [31].\n",
       " 2.2 Self-training Approach\n",
       " We adopt the pseudo-labeling strategy of Kahn et al. (2020; [ 13]) and Synnaeve et al. (2020; [ 2]).\n",
       " This ﬁrst trains an initial acoustic model on the available labeled data and then labels the unlabeled\n",
       " data with the initial model as well as a language model in a step we call pseudo-labeling. Finally, a\n",
       " new acoustic model is trained on the pseudo-labeled data as well as the original labeled data.\n",
       " Previous work considered multiple rounds of pseudo-labeling where the labeling step is repeated with\n",
       " each new model to train another model [ 14]. While iterative pseudo-labeling is more accurate, we\n",
       " opt for a single iteration which is computationally less demanding while still enabling us to reason\n",
       " about whether unsup,metadata = {'page': 2, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = fc0894f6-1e31-415d-b218-545804e50970, next_node = e244e260-6769-4923-93ce-d7c189b24939, parent_node = None, child_node = []),\n",
       " TextNode(id = e244e260-6769-4923-93ce-d7c189b24939,text = ervised pre-training and pseudo-labeling are complementary. Another line of\n",
       " work investigated ﬁltering the resulting pseudo-labeled data to match the distribution of the original\n",
       " labeled data [15]. Both methods may improve results and we leave them to future work.\n",
       " 2.3 Combining the two Approaches\n",
       " To combine the approaches, we replace the initial model for pseudo-labeling with a pre-trained model.\n",
       " The resulting training pipeline is as follows: we ﬁrst pre-train a wav2vec 2.0 model on the unlabeled\n",
       " data, ﬁne-tune it on the available labeled data, use the model to label the unlabeled data, and ﬁnally\n",
       " use the pseudo-labeled data to train the ﬁnal model. In our experiments, we also consider a variant\n",
       " where we ﬁne-tune the original wav2vec 2.0 model on the pseudo-labeled data.\n",
       " 3 Experimental Setup\n",
       " 3.1 Datasets\n",
       " As unlabeled data for pre-training and self-training we consider the speech audio of the Librispeech\n",
       " corpus (LS-960; [ 32]) without transcriptions containing 960h of audio as well as the audio data of\n",
       " LibriV ox (LV-60k). For the latter we follow the pre-processing of Kahn et al. (2020; [ 33]) resulting\n",
       " in 53.2k hours of audio. We consider ﬁve labeled data setups: all 960h of transcribed Librispeech, the\n",
       " train-clean-100 subset comprising 100h, as well as the Libri-light limited resource training subsets of\n",
       " train-10h (10h), train-1h (1h), and train-10min (10min). We evaluate on the standard Librispeech\n",
       " dev-other/clean and test-clean/other sets.\n",
       " 2,metadata = {'page': 2, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 560ab392-b850-4494-8762-121c22bbca02, next_node = c9795a2e-555d-447f-b589-c6f8b870de15, parent_node = None, child_node = []),\n",
       " TextNode(id = c9795a2e-555d-447f-b589-c6f8b870de15,text = 3.2 Pre-trained models\n",
       " Pre-trained models are implemented in fairseq [ 34] and we obtain them from the public fairseq\n",
       " github repository.2The repository provides ﬁne-tuned models for the ﬁve labeled data setups we\n",
       " consider (§ 3.1). We experiment with the LARGE conﬁguration comprising 24 transformer blocks\n",
       " with model dimension 1,024, inner dimension 4,096 and 16 attention heads, comprising a total of\n",
       " about 300M parameters. The feature encoder contains seven blocks and the temporal convolutions in\n",
       " each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2), resulting\n",
       " in a receptive ﬁeld of about 25ms and a stride of about 20ms. After pre-training on the unlabeled data,\n",
       " this model is ﬁne-tuned on the labeled data using Connectionist Temporal Classiﬁcation (CTC; [ 35])\n",
       " and a letter-based output vocabulary.\n",
       " 3.3 Self-training\n",
       " We pseudo-label the audio data of either LS-960 or LV-60k using wav2vec 2.0 LARGE ﬁne-tuned on\n",
       " different labeled data splits. For labeling, we follow the two-pass rescoring procedure of Synnaeve et\n",
       " al. (2020; [ 2]): ﬁrst, we generate a list of candidate transcriptions by combining wav2vec 2.0 and the\n",
       " standard Librispeech 4-gram language model during beam-search with beam 800. Next, the n-best\n",
       " list is pruned to the 50 highest scoring entries and then rescored with a Transformer LM trained on\n",
       " the Librispeech language corpus [ 36,2]. The Transformer LM has 20 blocks with model dimension\n",
       " 1,280, inner dimension 6,144 and 16 attention heads. The n-gram model obtains perplexity 150.3 on\n",
       " the development set and the Transformer language model 49.2. We found this to be more efﬁcient\n",
       " than directly integrating the Transformer LM into beam search at little loss in accuracy. Decoding\n",
       " and rescoring hyper-parameters are tuned on dev-other of Librispeech for each experiment using a\n",
       " random parameter search. The LM weight and the word insertion penalty [ 2] is tuned by randomly\n",
       " sampling values in the range of [0, 5] and [-5, 5] over 128 trials.\n",
       " 3.4 Final Model\n",
       " We follow Synnaeve et al. (2020; [ 2]) and train a Transformer-based sequence to sequence model\n",
       " with log-Mel ﬁlterbank inputs after pseudo-labeling using wav2letter++ [ 37]. The encoder uses a\n",
       " convolutional frontend containing 4 layers of temporal convolutions with kernel width 3, followed\n",
       " by 36 Transformer blocks with model dimension 768, 4 attention heads and feed-forward network\n",
       " (FFN) dimension 3072 [28, 2]. The model contains about 300M parameters.\n",
       " We use a 10k word p,metadata = {'page': 3, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = e244e260-6769-4923-93ce-d7c189b24939, next_node = bc6d924b-f890-4416-a86e-e8cdecd7baec, parent_node = None, child_node = []),\n",
       " TextNode(id = bc6d924b-f890-4416-a86e-e8cdecd7baec,text = iece output vocabulary computed from the training transcriptions if the whole\n",
       " Librispeech training set is used as labeled data [ 38]. Otherwise, we switch to the 5k WP estimated on\n",
       " the train-clean-100 transcriptions [ 14,8]. Language models are incorporated similar to § 3.3. We\n",
       " use a 4-gram language model and then rescore with a Transformer LM. The beam size used in both\n",
       " decoding and rescoring is 50.\n",
       " 4 Results\n",
       " 4.1 Low-Resource Labeled Data\n",
       " Pre-training has been shown to be very effective in both high- and low-resource labeled training data\n",
       " setups whereas self-training has been most effective when at least a moderate amount of labeled\n",
       " data is available (\u0015100h; [ 14,15]). To get a sense of whether the combination of both methods\n",
       " can be even more effective, we start with experiments on the Libri-light setups with 10min, 1h and\n",
       " 10h of labeled data. For pre-training and pseudo-labeling we use either the 960h of Librispeech\n",
       " without transcriptions or the 53.2k hours of LibriV ox (§ 3.1). As baseline we consider wav2vec 2.0\n",
       " pre-trained on Librispeech and ﬁne-tuned on one of the labeled data splits.\n",
       " We use the publicly available wav2vec 2.0 models to pseudo-label (ST) the unlabeled data and then\n",
       " evaluate two options to train the ﬁnal model on the resulting labels: one is to train a new sequence\n",
       " to sequence model from random initialization with a word-piece vocabulary (s2s scratch) following\n",
       " Synnaeve et al. (2019; [ 2]; § 3.4). Another option is to ﬁne-tune wav2vec 2.0 on the pseudo-labeled\n",
       " data with CTC and a letter-based vocabulary (ctc ft).\n",
       " 2https://github.com/pytorch/fairseq/tree/master/examples/wav2vec\n",
       " 3,metadata = {'page': 3, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = c9795a2e-555d-447f-b589-c6f8b870de15, next_node = 86911da8-6bff-4ec3-8788-ef7f82a756ef, parent_node = None, child_node = []),\n",
       " TextNode(id = 86911da8-6bff-4ec3-8788-ef7f82a756ef,text = Table 1: WER on the Librispeech dev and test sets for the Libri-light low-resource labeled data setups\n",
       " of 10 min, 1 hour and 10 hours. As unlabeled data we use the audio of Librispeech (LS-960) or the\n",
       " larger LibriV ox (LV-60k). ST (s2s scratch) trains a sequence to sequence model with a word-piece\n",
       " vocabulary on the pseudo-labeled data from random initialization while as ST (ctc ft) ﬁne-tunes\n",
       " wav2vec 2.0 with the pseudo-labels using CTC and a letter-based vocabulary. All results are with\n",
       " language models at inference time.\n",
       " ModelUnlbld dev test\n",
       " data clean other clean other\n",
       " 10 min labeled\n",
       " Discr. BERT [27] LS-960 15.7 24.1 16.3 25.2\n",
       " wav2vec 2.0 [24] LS-960 6.6 10.6 6.8 10.8\n",
       " + ST (s2s scratch) LS-960 4.1 7.0 5.0 8.1\n",
       " + ST (ctc ft) LS-960 3.6 6.6 4.0 7.2\n",
       " wav2vec 2.0 [24] LV-60k 5.0 8.4 5.2 8.6\n",
       " + ST (s2s scratch) LV-60k 2.6 4.7 3.1 5.4\n",
       " + ST (ctc ft) LV-60k 2.8 4.6 3.0 5.2\n",
       " 1h labeled\n",
       " Discr. BERT [27] LS-960 8.5 16.4 9.0 17.6\n",
       " wav2vec 2.0 [24] LS-960 3.8 7.1 3.9 7.6\n",
       " + ST (s2s scratch) LS-960 2.9 5.6 3.4 6.6\n",
       " + ST (ctc ft) LS-960 2.8 5.5 3.1 6.3\n",
       " 10h labeled\n",
       " Discr. BERT [27] LS-960 5.3 13.2 5.9 14.1\n",
       " IPL [14] LS-960 23.5 25.5 24.4 26.0\n",
       " wav2vec 2.0 [24] LS-960 2.9 5.7 3.2 6.1\n",
       " + ST (s2s scratch) LS-960 2.5 5.1 3.5 5.9\n",
       " + ST (ctc ft) LS-960 2.6 5.2 2.9 5.7\n",
       " Table 1 shows that the combination of pre-training and self-training (wav2vec 2.0 + ST) outperforms\n",
       " pre-training alone (wav2vec 2.0) across all low-resource setups. It also achieves a very large\n",
       " improvement over iterative pseudo-labeling [ 14] in the 10h labeled setup. This is because the initial\n",
       " model is much stronger due to pre-training and it is very difﬁcult to train a good supervised-only\n",
       " model on just 10h of labeled data.\n",
       " With just 10 minutes of labeled data, the combination of pre-training and pseudo-labeling with\n",
       " LibriV ox achieves WER 5.2% on test-other. Using Librispeech (LS-960) as unlabeled data and 10\n",
       " minutes of labeled data, wav2vec 2.0 + ST achieves 4.0%/7.2% WER on test-clean/other compared\n",
       " to 4.2%/8.6% for the best known pseudo-labeling approach [ 15] which uses 100 hours of labeled\n",
       " data. More unlabeled data leads to large improvements, reducing WER from 4.0%/7.2% for LS-960\n",
       " to 3.0%/5.2% for LV-60k, a relative WER reduction of 25-28%. But increasing the amount of\n",
       " labeled data without more unlabeled data leads to diminishing returns - an issue we return to in § 5.\n",
       " Fine-tuning (ctc ft) generally outperforms from scratch training of a sequence to sequence model\n",
       " with a WP vocabulary (s2s scratch). This is likel,metadata = {'page': 4, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = bc6d924b-f890-4416-a86e-e8cdecd7baec, next_node = b685801c-da51-4b60-890f-1b2ce3a0333a, parent_node = None, child_node = []),\n",
       " TextNode(id = b685801c-da51-4b60-890f-1b2ce3a0333a,text = y because the model can leverage the pre-trained\n",
       " representations.\n",
       " 4.2 High-Resource Labeled Data\n",
       " Next, we evaluate performance with more labeled data. We consider the 100h clean subset of\n",
       " Librispeech as well as all 960h of labeled data in Librispeech. Table 2 shows that LS-960 as unlabeled\n",
       " data is not enough to outperform the baseline when 100h of labeled data is available. However,\n",
       " performance improves when using the much larger LV-60k, achieving a 10% relative WER reduction\n",
       " on test-other over wav2vec 2.0.\n",
       " When using the full Librispeech benchmark as labeled data, combining wav2vec 2.0 and pseudo-\n",
       " labeling achieves WER 1.5%/3.1%. This result was achieved with a strong sequence to sequence\n",
       " 4,metadata = {'page': 4, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 86911da8-6bff-4ec3-8788-ef7f82a756ef, next_node = bc59d590-0baf-4eb2-ad4d-1872598caa8c, parent_node = None, child_node = []),\n",
       " TextNode(id = bc59d590-0baf-4eb2-ad4d-1872598caa8c,text = Table 2: WER on Librispeech when using the clean 100h subset as labeled data or all 960h of\n",
       " Librispeech (cf. Table 1). Prior work used 860 unlabeled hours (LS-860) but the total with labeled\n",
       " data is 960h and comparable to our setup.\n",
       " ModelUnlbld dev test\n",
       " data clean other clean other\n",
       " 100h labeled\n",
       " Discr. BERT [27] LS-960 4.0 10.9 4.5 12.1\n",
       " ST [13] LS-860 5.4 19.0 5.8 20.1\n",
       " IPL [14] LS-860 5.0 8.0 5.6 9.0\n",
       " Noisy student [15] LS-860 3.9 8.8 4.2 8.6\n",
       " wav2vec 2.0 [24] LS-960 2.1 4.8 2.3 5.0\n",
       " + ST (s2s scratch) LS-960 2.3 4.6 2.7 5.4\n",
       " + ST (ctc ft) LS-960 2.2 4.6 2.4 5.0\n",
       " IPL [14] LV-60k 3.19 6.14 3.72 7.11\n",
       " wav2vec 2.0 [24] LV-60k 1.9 4.0 2.0 4.0\n",
       " + ST (s2s scratch) LV-60k 1.4 2.8 1.9 3.8\n",
       " + ST (ctc ft) LV-60k 1.7 3.2 1.9 3.6\n",
       " 960h labeled\n",
       " Supervised\n",
       " SpecAugment [1] - - - 2.5 5.8\n",
       " ContextNet [3] - 1.9 3.9 1.9 4.1\n",
       " Conformer [4] - 2.1 4.3 1.9 3.9\n",
       " Semi-supervised\n",
       " IPL [14] LV-60k 1.85 3.26 2.10 4.01\n",
       " Noisy Student [15] LV-60k 1.6 3.4 1.7 3.4\n",
       " wav2vec 2.0 [24] LV-60k 1.6 3.0 1.8 3.3\n",
       " + ST (s2s scratch) LV-60k 1.1 2.7 1.5 3.1\n",
       " + ST (ctc ﬁne-tune) LV-60k 1.6 2.9 1.8 3.3\n",
       " model trained from scratch. While less effective than ﬁne-tuning with CTC on smaller setups, a\n",
       " powerful sequence to sequence model excels in this larger setting since the decoder part of the model,\n",
       " which acts in part like a language model, does not overﬁt. The lower performance of ﬁne tuning is\n",
       " likely due to CTC not being as competitive as more elaborate sequence to sequence models when a\n",
       " lot of pseudo-labeled data is available [2].\n",
       " 4.3 Results without a Language Model at Inference Time\n",
       " Table 3 shows that combined training models have very good performance even without a language\n",
       " model. This is because the language model used during pseudo-labeling was partly distilled into\n",
       " the pseudo-labeled data [ 2]. This effect is particularly striking for the 10 min labeled setup without\n",
       " LM where wav2vec 2.0 + ST (s2s scratch) reduces the WER of the baseline (wav2vec 2.0 - LM) by\n",
       " 83% relative on test-other. As more labeled data becomes available, the performance of the acoustic\n",
       " model without a language model improves but there is still a clear effect of self-trained models having\n",
       " distilled the language model. Generally, the sequence to sequence model in the (s2s scratch) setting\n",
       " is better able to distill the language model used at pseudo-labeling time compared to the CTC model\n",
       " used in ﬁne-tuning.\n",
       " 5 Analysis\n",
       " We previously saw that improvements decreased with more labeled data (§ 4.1). To better understand\n",
       " this, we perform an experim,metadata = {'page': 5, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = b685801c-da51-4b60-890f-1b2ce3a0333a, next_node = c3c29798-f5e4-4628-b8dd-58f23e4cfa22, parent_node = None, child_node = []),\n",
       " TextNode(id = c3c29798-f5e4-4628-b8dd-58f23e4cfa22,text = ent on Librispeech where we consider data setups with a ﬁxed ratio\n",
       " between the unlabeled and labeled data. Table 4 shows that relative improvements are a function\n",
       " of the amount of unlabeled data relative to the labeled data, rather than the amount of labeled data\n",
       " alone. Table 1 showed much larger improvements for the 10 min labeled split but with a ﬁxed ratio\n",
       " 5,metadata = {'page': 5, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = bc59d590-0baf-4eb2-ad4d-1872598caa8c, next_node = 22616094-fe50-4785-b0aa-a85fcb66d1f6, parent_node = None, child_node = []),\n",
       " TextNode(id = 22616094-fe50-4785-b0aa-a85fcb66d1f6,text = Table 3: WER on Librispeech with and without a language model (LM) for 10 min, and 960h of\n",
       " labeled data and LibriV ox as unlabeled data.\n",
       " Modeldev test\n",
       " clean other clean other\n",
       " 10 min labeled\n",
       " wav2vec 2.0 [24] 5.0 8.4 5.2 8.6\n",
       " - LM 38.3 41.0 40.2 38.7\n",
       " wav2vec 2.0 + ST (s2s scratch) 2.6 4.7 3.1 5.4\n",
       " - LM 3.3 5.9 3.7 6.5\n",
       " wav2vec 2.0 + ST (ctc ft) 2.8 4.6 3.0 5.2\n",
       " - LM 4.2 6.9 4.3 7.2\n",
       " 960h labeled\n",
       " wav2vec 2.0 [24] 1.6 3.0 1.8 3.3\n",
       " - LM 2.1 4.5 2.2 4.5\n",
       " wav2vec 2.0 + ST (s2s scratch) 1.1 2.7 1.5 3.1\n",
       " - LM 1.3 3.1 1.7 3.5\n",
       " wav2vec 2.0 + ST (ctc ft) 1.6 2.9 1.8 3.3\n",
       " - LM 1.7 3.6 1.9 3.9\n",
       " Table 4: The main driver of performance is the ratio between labeled and unlabeled data. We add 8.6\n",
       " times as much unlabeled data to each labeled setup. Results are on dev-other with an n-gram model\n",
       " and subsets of LS-960 as unlabeled data.\n",
       " labeled unlab dev-other % change\n",
       " wav2vec 2.0 10min 86 min 12.9\n",
       " + ST 10min 86 min 12.0 7%\n",
       " wav2vec 2.0 1h 8.6h 8.5\n",
       " + ST 1h 8.6h 7.6 11%\n",
       " wav2vec 2.0 10h 86h 6.9\n",
       " + ST 10h 86h 6.5 6%\n",
       " wav2vec 2.0 100h 860h 5.7\n",
       " + ST 100h 860h 5.3 7%\n",
       " of labeled and unlabeled data, the relative improvement is comparable to the 100h labeled setup\n",
       " (Table 2).\n",
       " 6 Conclusion\n",
       " Unsupervised pre-training and pseudo-labeling are complementary for speech recognition. This\n",
       " enables building speech recognition systems with as little as 10 minutes of transcribed speech with\n",
       " word error rates that only a year ago were reserved to the best systems trained on 960 hours of labeled\n",
       " data.\n",
       " References\n",
       " [1]D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V . Le. Specaugment:\n",
       " A simple data augmentation method for automatic speech recognition. In Proc. of Interspeech ,\n",
       " 2019.\n",
       " [2]G. Synnaeve et al. End-to-end ASR: from Supervised to Semi-Supervised Learning with\n",
       " Modern Architectures. arXiv , abs/1911.08460, 2019.\n",
       " 6,metadata = {'page': 6, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = c3c29798-f5e4-4628-b8dd-58f23e4cfa22, next_node = 4f522bb7-6345-4729-b3b9-fedd8ff5f793, parent_node = None, child_node = []),\n",
       " TextNode(id = 4f522bb7-6345-4729-b3b9-fedd8ff5f793,text = [3]W. Han et al. Contextnet: Improving convolutional neural networks for automatic speech\n",
       " recognition with global context. arXiv , 2020.\n",
       " [4]A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, and et al. Conformer: Convolution-\n",
       " augmented transformer for speech recognition. arXiv , 2020.\n",
       " [5]M. P. Lewis, G. F. Simon, and C. D. Fennig. Ethnologue: Languages of the world, nineteenth\n",
       " edition. Online version: http://www.ethnologue.com , 2016.\n",
       " [6]A. H. Liu, H.-Y . Lee, and L.-S. Lee. Adversarial training of end-to-end speech recognition\n",
       " using a criticizing language model. arXiv , 2018.\n",
       " [7]M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and J. ˇCernocký. Semi-supervised\n",
       " sequence-to-sequence asr using unpaired speech and text. arXiv , 2019.\n",
       " [8]W.-N. Hsu, A. Lee, G. Synnaeve, and A. Hannun. Semi-supervised speech recognition via local\n",
       " prior matching. arXiv , 2020.\n",
       " [9]H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Trans.\n",
       " on Inform. Theory , 1965.\n",
       " [10] D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In Proc.\n",
       " of ACL , 1995.\n",
       " [11] E. Riloff. Automatically generating extraction patterns from untagged text. In Proc. of AAAI ,\n",
       " 1996.\n",
       " [12] S. H. K. Parthasarathi and N. Strom. Lessons from building acoustic models with a million\n",
       " hours of speech. arXiv , 2019.\n",
       " [13] J. Kahn, A. Lee, and A. Hannun. Self-training for end-to-end speech recognition. In Proc. of\n",
       " ICASSP , 2020.\n",
       " [14] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert. Iterative\n",
       " pseudo-labeling for speech recognition. arXiv , 2020.\n",
       " [15] D. S. Park, Y . Zhang, Y . Jia, W. Han, C.-C. Chiu, and et al. Improved noisy student training for\n",
       " automatic speech recognition. arXiv , 2020.\n",
       " [16] A. v. d. Oord, Y . Li, and O. Vinyals. Representation learning with contrastive predictive coding.\n",
       " arXiv , abs/1807.03748, 2018.\n",
       " [17] S. Schneider, A. Baevski, R. Collobert, and M. Auli. wav2vec: Unsupervised pre-training for\n",
       " speech recognition. In Proc. of Interspeech , 2019.\n",
       " [18] A. Baevski, S. Schneider, and M. Auli. vq-wav2vec: Self-supervised learning of discrete speech\n",
       " representations. In Proc. of ICLR , 2020.\n",
       " [19] Y .-A. Chung, W.-N. Hsu, H. Tang, and J. R. Glass. An unsupervised autoregressive model for\n",
       " speech representation learning. arXiv , abs/1904.03240, 2019.\n",
       " [20] D. Jiang, X. Lei, W. Li, N. Luo, Y . Hu, and et al. Improving transformer-based speech\n",
       " recognition using unsupervised pre-training. arXiv , abs/1910.09932, 2019.\n",
       " [,metadata = {'page': 7, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 22616094-fe50-4785-b0aa-a85fcb66d1f6, next_node = faa7d261-4836-4510-8aee-61f327c891ea, parent_node = None, child_node = []),\n",
       " TextNode(id = faa7d261-4836-4510-8aee-61f327c891ea,text = 21] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. v. d. Oord. Learning robust and multilin-\n",
       " gual speech representations. arXiv , 2020.\n",
       " [22] M. Rivière, A. Joulin, P.-E. Mazaré, and E. Dupoux. Unsupervised pretraining transfers well\n",
       " across languages. arXiv , abs/2002.02848, 2020.\n",
       " [23] W. Wang, Q. Tang, and K. Livescu. Unsupervised pre-training of bidirectional speech encoders\n",
       " via masked reconstruction. arXiv , 2020.\n",
       " [24] A. Baevski, H. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: A framework for self-supervised\n",
       " learning of speech representations. In Proc. of NeurIPS , 2020.\n",
       " [25] J. Du, E. Grave, B. Gunel, V . Chaudhary, O. Celebi, and et al. Self-training improves pre-training\n",
       " for natural language understanding. arXiv , 2020.\n",
       " [26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\n",
       " transformers for language understanding. arXiv , abs/1810.04805, 2018.\n",
       " [27] A. Baevski, M. Auli, and A. Mohamed. Effectiveness of self-supervised pre-training for speech\n",
       " recognition. arXiv , abs/1911.03912, 2019.\n",
       " 7,metadata = {'page': 7, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 4f522bb7-6345-4729-b3b9-fedd8ff5f793, next_node = f4b17c76-67c8-4b5c-aeab-be2a20868e41, parent_node = None, child_node = []),\n",
       " TextNode(id = f4b17c76-67c8-4b5c-aeab-be2a20868e41,text = [28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, and et al. Attention is all you need.\n",
       " InProc. of NIPS , 2017.\n",
       " [29] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE\n",
       " Trans. Pattern Anal. Mach. Intell. , 2011.\n",
       " [30] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.\n",
       " arXiv , abs/1611.01144, 2016.\n",
       " [31] S. Dieleman, A. v. d. Oord, and K. Simonyan. The challenge of realistic music generation:\n",
       " modelling raw audio at scale. arXiv , 2018.\n",
       " [32] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public\n",
       " domain audio books. In Proc. of ICASSP , pages 5206–5210. IEEE, 2015.\n",
       " [33] J. Kahn and et al. Libri-light: A benchmark for asr with limited or no supervision. In Proc. of\n",
       " ICASSP , 2020.\n",
       " [34] M. Ott et al. fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL Sys.\n",
       " Demo. , 2019.\n",
       " [35] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber. Connectionist temporal classiﬁcation:\n",
       " labelling unsegmented sequence data with recurrent neural networks. In ICML , 2006.\n",
       " [36] A. Baevski and M. Auli. Adaptive input representations for neural language modeling. In Proc.\n",
       " of ICLR , 2018.\n",
       " [37] V . Pratap et al. Wav2letter++: A fast open-source speech recognition system. In Proc. of\n",
       " ICASSP , 2019.\n",
       " [38] T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword\n",
       " tokenizer and detokenizer for neural text processing. In Proc. of EMNLP Sys. Demo. , 2018.\n",
       " 8,metadata = {'page': 8, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = faa7d261-4836-4510-8aee-61f327c891ea, next_node = None, parent_node = None, child_node = [])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "loader = PDFLoader('datasets/papers_pdf')\n",
    "documents = loader.get_documents()\n",
    "images = loader.get_images(path = loader.path)\n",
    "bridge = DocumentBridge(documents, context = context)\n",
    "nodes = bridge.nodes(chunk_size = 2500)\n",
    "nodes\n",
    "#For images save surrounding image context for context + gpt/blip interpretation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399ffa9-9d01-4ce6-b19e-2f4120e3eeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id = 42e3a831-8a50-49d5-bbf5-bbf598191bc1, name = Self-training and Pre-training are Complementary, metadata = {'page': 1, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = a577ea8a-8b9c-4821-b454-39692cbba28a, name = Self-training and Pre-training are Complementary, metadata = {'page': 2, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = 6b285da8-c08d-4bb3-8bc7-8bd2e2366ff4, name = Self-training and Pre-training are Complementary, metadata = {'page': 3, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = 55b68b48-e5a7-4fba-91e4-a03ffcf8ad0e, name = Self-training and Pre-training are Complementary, metadata = {'page': 4, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = bb98ce1f-8fe0-4054-b065-4b66e50b7538, name = Self-training and Pre-training are Complementary, metadata = {'page': 5, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = b7b97623-a97f-4749-ac8b-b6dad3d6ebdf, name = Self-training and Pre-training are Complementary, metadata = {'page': 6, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = 12d0fec9-df96-493b-9866-cb912fbfa68c, name = Self-training and Pre-training are Complementary, metadata = {'page': 7, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = 50e9801d-c189-4d6a-b4f4-d329ca25044e, name = Self-training and Pre-training are Complementary, metadata = {'page': 8, '/Author': '', '/CreationDate': 'D:20201023010016Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20201023010016Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', '/Producer': 'pdfTeX-1.40.21', '/Subject': '', '/Title': '', '/Trapped': '/False'}, n_nodes = 0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "store = DocumentStore(documents)\n",
    "ids = store.ids()\n",
    "store.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdd428-3ac7-4ba4-9284-b263f98054e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "# Maybe I can make it so the bridge knows if an object is a Node or a Doc when inputting it and serves for both. \n",
    "documents = store.get()\n",
    "bridge = DocumentBridge(documents, context = context)\n",
    "nodes = bridge.nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2411c4-df1a-4a5b-8259-6666f6ef15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfe06c-ab4c-45d2-a68c-2aa5cca8896b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4a378-177c-43b0-8b05-ed573ec0f410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3c177-be06-47fd-ac69-d2ce2c3efd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
