{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8404bfc-069b-49b4-b140-49c850418b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc954c-662d-4ac0-96f2-84bcb70a8950",
   "metadata": {},
   "source": [
    "# Loaders\n",
    "\n",
    "> A module for importing data and converting it to a processable output for the most typical file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef0a0e-b785-4395-8f52-8c42e9998aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import random\n",
    "from typing import List\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48c660-a2f3-4984-b1ec-43f15d7d2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev_rag.store import *\n",
    "from nbdev_rag.base import *\n",
    "from nbdev_rag.context import *\n",
    "from nbdev_rag.llm import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f679303-536a-42ac-a5a0-e62315cbaccc",
   "metadata": {},
   "source": [
    "#|hide\n",
    "We have a set of PDF files we previously downloaded to test out and create a PDF loader. We will try to create a loader that extract both images and text in an structured way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099616f-6a84-4c8c-ac35-09f4df9cbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#For simplicity lets start with accepting a List. \n",
    "class PDFLoader:\n",
    "    \"\"\"Accepts a dir or single path and converts its contents into documents that can be later used for storage and retrieval\"\"\"\n",
    "    def __init__(self, path_dir: str):\n",
    "        self.path_dir = Path(path_dir)\n",
    "        if self.path_dir.is_dir():\n",
    "            self.paths = [path for path in self.path_dir.iterdir() if path]\n",
    "        else:\n",
    "            self.paths = [self.path_dir]\n",
    "        self.path = None\n",
    "        \n",
    "    def pdf_validator(self, path):\n",
    "        \"\"\"Tries to read the pdf and returns a Bool value with the result\"\"\"\n",
    "        try:\n",
    "            reader = PdfReader(path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "    def load_random_pdf(self):\n",
    "        \"\"\"Load a random pdf from the dataset. It loads pdfs until a valid one is found\"\"\"\n",
    "        valid_pdf_found = False\n",
    "        while not valid_pdf_found:  # Continue until a valid PDF is found\n",
    "            pdf_path = random.choice(self.paths)\n",
    "            is_valid = self.pdf_validator(pdf_path)\n",
    "            if is_valid:\n",
    "                reader = PdfReader(pdf_path)\n",
    "                valid_pdf_found = True\n",
    "                self.path = pdf_path\n",
    "                return reader\n",
    "            else:\n",
    "                pdf_path.unlink()\n",
    "                self.paths.remove(pdf_path)  # Remove the invalid path from the list\n",
    "        \n",
    "        if not valid_pdf_found:\n",
    "            return None\n",
    "    def load_pdf(self, path):\n",
    "        reader = PdfReader(path)\n",
    "        self.path = path\n",
    "        return reader\n",
    "    \n",
    "    def get_documents(self, path = None):\n",
    "        \"\"\"Get a List of Text Documents from a pdf Path.\"\"\"\n",
    "        documents = []\n",
    "        #Extracting text and storing it in documents\n",
    "        if path == None:\n",
    "            reader = self.load_random_pdf()\n",
    "        else:\n",
    "            reader = self.load_pdf(path)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            params = {\"metadata\": {**{\"page\": i + 1}, **reader.metadata}, \"text\": page.extract_text()}\n",
    "            if i == 0:\n",
    "                title = reader.metadata.get('title', None)\n",
    "                if title is None:\n",
    "                    title = params['text'].split('\\n')[0]        \n",
    "            if title is not None:\n",
    "                params[\"name\"] = title\n",
    "            doc = Document(**params)\n",
    "            documents.append(doc)\n",
    "        return documents\n",
    "    def get_images(self, path = None):\n",
    "        #Can add some metadata like what page and location was found on. \n",
    "        #Create Image Node with that kind of info. \n",
    "        if path == None:\n",
    "            reader = self.load_random_pdf()\n",
    "        else:\n",
    "            reader = self.load_pdf(path)\n",
    "        images = []\n",
    "        for count, page in enumerate(reader.pages):\n",
    "            for image_file_object in page.images:\n",
    "                image = Image.open(BytesIO(image_file_object.data))\n",
    "                images.append(image)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068f724-47ea-4192-9986-d340776d8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DocumentBridge:\n",
    "    \"\"\"Class for connecting a list of documents into its corresponding Nodes and relationships\"\"\"\n",
    "    def __init__(self, documents: List, context: ModelContext):\n",
    "        if isinstance(documents, List):\n",
    "            self.documents = documents\n",
    "        else:\n",
    "            raise \"You have to include a List of documents\"\n",
    "        self.context = context\n",
    "    def nodes(self, chunk_size = 1024) -> List[TextNode]:\n",
    "        \"\"\"Brige a series of Documents into nodes linked by the end and start of the prev and next document. Great for linking together complex docs with structure\n",
    "        such as pages or other info extracted first on a Document basis.\"\"\"\n",
    "        doc_nodes_list = [doc.create_nodes_from_doc(self.context, chunk_size = chunk_size) for doc in self.documents]\n",
    "        for i, node_list in enumerate(doc_nodes_list):\n",
    "            if i == 0:\n",
    "                node_list[-1].next_node = doc_nodes_list[i + 1][0].id\n",
    "            else:\n",
    "                if i < len(doc_nodes_list) - 1:\n",
    "                    node_list[-1].next_node = doc_nodes_list[i + 1][0].id\n",
    "                node_list[0].prev_node = doc_nodes_list[i - 1][-1].id\n",
    "        nodes = [node for node_list in doc_nodes_list for node in node_list]\n",
    "        return nodes\n",
    "        \n",
    "    def join(self) -> Document:\n",
    "        \"\"\"Bridges a series of Documents into a single document. Great for storing sub-documents into a single one. Keeps some metadata of the documents into one. \"\"\"\n",
    "        #Store metadata about length, pages etc. For the later processing to be better. Maybe metadata about where each page started and ended in terms of characters could be good. \n",
    "        #see tradeoffs between this and diff docs pointing to a single reference. \n",
    "        #In reality in the conversion to nodes all the info is kept. We can post-process there. \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448a0f8-8817-4b41-9d1e-13e7f57d1c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b2564cdefd469499278afb965eb788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map = \"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "llm = LLM(model = model, tokenizer = tokenizer)\n",
    "context = ModelContext(llm = llm, embedding = embedding_model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b246bb-f801-412d-96d3-54d06dcf39b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id = b724179a-84d8-409c-ae48-7af22b0529ab,text = Are you talking to [‘xem’] or [‘x’, ‘em’]? On\n",
       " Tokenization and Addressing Misgendering in LLMs\n",
       " with Pronoun Tokenization Parity\n",
       " Anaelia Ovalle∗‡Ninareh Mehrabi†Palash Goyal†\n",
       " Jwala Dhamala†‡Kai-Wei Chang‡†Richard Zemel†\n",
       " Aram Galstyan†Rahul Gupta†\n",
       " ‡University of California, Los Angeles†Amazon Alexa\n",
       " Abstract\n",
       " A large body of NLP research has documented the ways gender biases manifest\n",
       " and amplify within large language models (LLMs), though this research has pre-\n",
       " dominantly operated within a gender binary-centric context. A growing body\n",
       " of work has identified the harmful limitations of this gender-exclusive framing;\n",
       " many LLMs cannot correctly and consistently refer to persons outside the gender\n",
       " binary, especially if they use neopronouns. While data scarcity has been identified\n",
       " as a possible culprit, the precise mechanisms through which it influences LLM\n",
       " misgendering remain underexplored. Our work addresses this gap by studying data\n",
       " scarcity’s role in subword tokenization and, consequently, the formation of LLM\n",
       " word representations. We uncover how the Byte-Pair Encoding (BPE) tokenizer,\n",
       " a backbone for many popular LLMs, contributes to neopronoun misgendering\n",
       " through out-of-vocabulary behavior. We introduce pronoun tokenization parity\n",
       " (PTP), a novel approach to reduce LLM neopronoun misgendering by preserv-\n",
       " ing a token’s functional structure. We evaluate PTP’s efficacy using pronoun\n",
       " consistency-based metrics and a novel syntax-based metric. Through several con-\n",
       " trolled experiments, finetuning LLMs with PTP improves neopronoun consistency\n",
       " from 14.5% to 58.4%, highlighting the significant role tokenization plays in LLM\n",
       " pronoun consistency.\n",
       " 1 Introduction\n",
       " Gender bias in natural language processing (NLP) has been widely studied in the context of binary\n",
       " gender, however mitigating harmful biases for underrepresented gender minorities remains an active\n",
       " area of research Sun et al. [2019], Stanczak and Augenstein [2021]. As demonstrated in prior\n",
       " research Dev et al. [2021], Ovalle et al. [2023], Hossain et al. [2023], large language models (LLMs)\n",
       " struggle with correctly addressing individuals using non-binary pronouns, especially when they\n",
       " are neopronouns (e.g., xe,ey)2. In addition to their respective findings, these works necessarily\n",
       " highlight the connection between LLM misgendering and data scarcity; neopronouns are often\n",
       " severely underrepresented in a pretraining text corpus, thus impacting the LLMs ability to learn how\n",
       " ∗Corresponding Author, anaelia@cs.ucla.edu\n",
       " 2https:,metadata = {'page': 1, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = None, next_node = cc8110ca-9b52-4b7b-b90a-e3ba3aea2c80, parent_node = None, child_node = []),\n",
       " TextNode(id = cc8110ca-9b52-4b7b-b90a-e3ba3aea2c80,text = //nonbinary.wiki/wiki/English_neutral_pronouns\n",
       " 37th Neural Information Processing Systems Queer in AI Workshop (NeurIPS Queer in AI 2023).arXiv:2312.11779v2  [cs.CL]  21 Dec 2023,metadata = {'page': 1, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = b724179a-84d8-409c-ae48-7af22b0529ab, next_node = 2a167d5b-5de5-44f9-9747-f42c649cfb19, parent_node = None, child_node = []),\n",
       " TextNode(id = 2a167d5b-5de5-44f9-9747-f42c649cfb19,text = Hisresume is impressive!Zyrresume is impressive!Xirresume is impressive!Eirresume is impressive!Faerresume is impressive!Herresume is impressive!\n",
       " [His, resume, is, impressive][Zy, r, resume, is, impressive][X, ir, resume, is, impressive][E, ir, resume, is, impressive][Fa, er, resume, is, impressive][Her, resume, is, impressive]\n",
       " Raw Text\n",
       " Byte-Pair Encoding (BPE) Tokenizer\n",
       " HighTerm Frequency\n",
       " Eir\n",
       " Xir\n",
       " Zyr\n",
       " FaerHer\n",
       " His\n",
       " NeopronounsBinary Pronouns\n",
       " LowTerm FrequencyBPE prioritizes frequently occurring terms during LLM vocabulary creation. Thus, neopronounsoften result in fragmented tokenization due to their rarity in the text corpus.Tokenized TextPretraining CorpusFigure 1: LLMs critically rely on their tokenizer’s predefined base vocabulary to form their repre-\n",
       " sentations. As BPE prioritizes constructing full tokens for frequently occurring terms, neopronoun\n",
       " sparsity in a text corpus impacts tokenization and its subsequent LLM representation. Neopronouns\n",
       " are fragmented into subword units while binary pronouns are represented as single tokens, thereby\n",
       " introducing syntactic challenges for LLMs which we investigate in this paper.\n",
       " to use them appropriately. However, investigation into the specific mechanisms by which data scarcity\n",
       " impacts LLM misgendering behavior remains limited. Our study tackles this research gap from a\n",
       " new angle, focusing on a fundamental aspect of LLM representation development: tokenization.\n",
       " Figure 1 illustrates how binary pronouns and the neopronoun xirare tokenized under Byte-Pair\n",
       " Encoding (BPE), today’s most common subword tokenizer used in popular LLMs such as GPT-\n",
       " 2 Radford et al. [2019], GPT-3 Brown et al. [2020], and LLaMA Touvron et al. [2023]. While binary\n",
       " pronouns ( herandhis) are tokenized as single units, neopronouns like xirandeirare fragmented\n",
       " into two subword tokens due to their infrequency within the tokenizer’s training corpus. This out-of-\n",
       " vocabulary (OOV) tokenization subsequently forces the LLM to rely on granular subword tokens\n",
       " to learn the neopronoun’s representation (embedding). As these subwords are present in multiple\n",
       " words, their embeddings incorporate information from these common words, making it challenging\n",
       " to distinguish the neopronoun in the model. This paper explores the impact this low-resource\n",
       " tokenization has on an LLM’s ability to correctly refer to a person’s neopronouns (i.e. pronoun\n",
       " consistency). Our experiments reveal that this fragmentation significantly impairs an LLM’s ability\n",
       " to correctly and consisten,metadata = {'page': 2, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = cc8110ca-9b52-4b7b-b90a-e3ba3aea2c80, next_node = afdeb52f-0d0b-4720-a1bd-34fc31b5d998, parent_node = None, child_node = []),\n",
       " TextNode(id = afdeb52f-0d0b-4720-a1bd-34fc31b5d998,text = tly use neopronouns.\n",
       " Guided by prior NLP literature detailing LLM syntactic challenges introduced by OOV , our paper\n",
       " introduces a novel mitigation strategy termed pronoun tokenization parity (PTP). PTP centers aligning,\n",
       " or establishing parity, between neopronoun and binary pronoun tokenization. We preserve the\n",
       " neopronoun’s functional structure as a pronoun by representing it as a single token for LLM input.\n",
       " We evaluate the efficacy of PTP both with typical pronoun-consistency metrics alongside a novel\n",
       " syntactic knowledge-based metric strongly associated with pronoun consistency. Furthermore, given\n",
       " the substantial training costs of LLMs and their resulting environmental impact, we present a cost-\n",
       " effective alternative which exploits an LLM’s existing grammatical knowledge to achieve substantial\n",
       " improvements in LLM pronoun consistency.\n",
       " Finetuning GPT-based models across carefully augmented neopronoun datasets show PTP providing\n",
       " up to 58.4% pronoun consistency, compared to 14.5% when traditionally finetuning without PTP\n",
       " (§5.1). When finetuning only the LLM’s lexical layer, a technique commonly seen in multilingual\n",
       " NLP for cross lingual transfer, it surpasses the performance of full fine-tuning for most of our models\n",
       " 2,metadata = {'page': 2, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 2a167d5b-5de5-44f9-9747-f42c649cfb19, next_node = 0f7046a9-a576-43e9-9fc2-0e4158c58949, parent_node = None, child_node = []),\n",
       " TextNode(id = 0f7046a9-a576-43e9-9fc2-0e4158c58949,text = Pronoun Case He She Xe\n",
       " Nominative [he] [she] [xe]\n",
       " Accusative [him] [her] [x, em]\n",
       " Pronominal Possessive [his] [her] [x, ir]\n",
       " Predicative Possessive [his] [hers] [x, irs]\n",
       " Reflexive [him, self] [her, self] [x, ir, self]\n",
       " Figure 2: BPE Tokenization of Binary Pronoun\n",
       " Cases and Neopronoun Cases for Xe.\n",
       " 0.0 0.5 1.0 1.5 2.0\n",
       " Fertility10k most common words \n",
       " (Google Trillion Word Corpus)\n",
       " Binary Pronouns\n",
       " NeopronounsFertility by T oken Category Figure 3: Fertility by pronoun category. Details\n",
       " and definitions of measurement found in §A.2.\n",
       " (75%) while simultaneously reducing training time by up to 21.5% (§5.2). Testing our methods at\n",
       " scale, we find that lexical fine-tuning consistently improves LLM pronoun consistency across model\n",
       " sizes, with smaller models experiencing the most improvements - even matching the performance of\n",
       " models twice their size (§5.3).\n",
       " 2 Background\n",
       " Gender-Inclusive NLP Gender biases have been studied across several NLP contexts, including\n",
       " machine translation Stanovsky et al. [2019], coreference resolution Rudinger et al. [2018], Zhao\n",
       " et al. [2018], and named entity recognition Mehrabi et al. [2019]. Recent works expand gender bias\n",
       " evaluations to harms unique to non-normative gender communities within LLMs Dev et al. [2021],\n",
       " Hossain et al. [2023], Ovalle et al. [2023], Nozza et al. [2022], Felkner et al. [2023], of QueerInAI\n",
       " et al. [2023]. Dev et al. [2021] examines non-binary gender biases in static and contextual language\n",
       " representations, highlighting how data limitations affect these embeddings. Similarly, Ovalle et al.\n",
       " [2023] explores misgendering and harmful responses related to gender disclosure using their TANGO\n",
       " framework, pointing to challenges in neopronoun consistency, possibly due to data scarcity. Hossain\n",
       " et al. [2023] corroborates these findings with an incontext-learning evaluation and analyses into\n",
       " LLM pretraining corpus statstics. Despite exploring various in-context learning strategies, they find\n",
       " persistent gaps between binary pronoun and neopronoun misgendering.\n",
       " While these studies collectively emphasize data scarcity’s impact on neopronouns, questions remain\n",
       " regarding the precise mechanisms through which data scarcity shapes neopronoun representations\n",
       " and subsequent LLM pronoun consistency. In this study, we investigate the pivotal role of BPE\n",
       " tokenization due to its inextricable link to both data scarcity and the construction of LLM word\n",
       " representations. We see how studying BPE behavior in resource-constrained settings yields valuab,metadata = {'page': 3, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = afdeb52f-0d0b-4720-a1bd-34fc31b5d998, next_node = 710b8793-643d-467e-9377-6f6325f85ea6, parent_node = None, child_node = []),\n",
       " TextNode(id = 710b8793-643d-467e-9377-6f6325f85ea6,text = le\n",
       " clues into LLM misgendering, offering a new approach towards addressing this issue.\n",
       " BPE Tokenization Byte-Pair Encoding (BPE) is a subword tokenization technique Sennrich et al.\n",
       " [2016] that constructs token vocabularies by iteratively merging frequently occurring adjacent token\n",
       " pairs until a predefined vocabulary size is reached. BPE relies on token frequency in the corpus to\n",
       " create these merge rules and does not assign a specific “unknown” token [UNK] to unseen words. In\n",
       " cases where a word is not present in the vocabulary, BPE decomposes it into subword units, with the\n",
       " most granular unit being individual characters. This method is designed to encompass both frequent\n",
       " and infrequent subword units, yet it remains solely influenced by text frequency and does not take\n",
       " into account inductive or semantic factors.\n",
       " 3 Low-Resource Challenges for BPE & LLMs\n",
       " The Out-of-Vocabulary Problem The Out-of-V ocabulary (OOV) problem refers to when an\n",
       " LLM encounters a word outside its predefined vocabulary, due to novelty or rarity in the training\n",
       " corpus. For LLMs using subword tokenizers like BPE, OOV words are broken down into multiple\n",
       " smaller tokens. BPE tokenization seems to reflect this OOV behavior with neopronouns like xem.\n",
       " Shown in Figure 2, the BPE tokenizer breaks xemdown into character and subword token. Unlike\n",
       " tokenization for binary pronouns heandshe, BPE does not treat it as single unit, indicating that\n",
       " the token cannot be constructed by the LLM’s predefined vocabulary Yehezkel and Pinter [2022].\n",
       " This phenomenon presents numerous challenges across NLP tasks, with notable documentation in\n",
       " machine translation Domingo et al. [2018], Huck et al. [2019], Araabi et al. [2022].\n",
       " 3,metadata = {'page': 3, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 0f7046a9-a576-43e9-9fc2-0e4158c58949, next_node = 21cd7ec8-2686-4fe5-9a63-2fe63eb56be9, parent_node = None, child_node = []),\n",
       " TextNode(id = 21cd7ec8-2686-4fe5-9a63-2fe63eb56be9,text = OOV and Syntactic Knowledge Gaps in LLM syntactic knowledge due to OOV behavior is\n",
       " well documented. Wang et al. [2019] highlight OOVs’ detrimental impact on part-of-speech (POS)\n",
       " discernment, resulting in high error rates for OOV words. Such challenges manifest across tasks\n",
       " reliant on syntactic understanding, such as POS tagging Wicaksono and Purwarianti [2010], Pinter\n",
       " et al. [2017], Wang et al. [2019], named entity recognition Da ˇrena and Suss [2020], Wang et al.\n",
       " [2022], and quality estimation for machine translation Domingo et al. [2018], Huck et al. [2019],\n",
       " Araabi et al. [2022]. The authors stress token-level comprehension’s importance in these tasks,\n",
       " underscoring the need to address OOV challenges for improved downstream performance.\n",
       " Rust et al. [2021] also highlights LLM sensitivity to tokenization through a comprehensive empirical\n",
       " analysis of token fragmentation across several languages. They introduce ‘fertility’, the average\n",
       " number of subwords produced per tokenized word, to quantify tokenizer impacts on LLM performance\n",
       " across benchmark NLP tasks: the closer fertility is to 1, the better the tokenizer performs. After\n",
       " undergoing BPE tokenization, neopronouns frequently decompose into multiple subword units,\n",
       " resulting in elevated fertility scores. These fertility scores are substantially higher than those found for\n",
       " binary pronouns, as depicted in Figure 3. Given the observed LLM sensitivity to tokenization reflected\n",
       " in POS tagging and dependency parsing by Rust et al. [2021], we posit that these decompositions\n",
       " adversely impact an LLM’s ability to handle neopronouns.\n",
       " OOV and Context Formation LLMs rely on context to make predictions and generate coherent\n",
       " text. Linguistic ambiguity may arise when splitting xem into the tokens xandem, reflected in\n",
       " challenges to use these subword units in their appropriate context. For instance, if pretraining data\n",
       " shows subtokens xandemas (parts of) nouns or abbreviations, using these tokens as pronouns\n",
       " reflects a less familiar context for the LLM Dev et al. [2021], thereby introducing confounding\n",
       " signals from many different contexts into one token. Furthermore, xemcombines the relatively rare\n",
       " character xand a frequently occurring character pair em. Since frequency in text plays a significant\n",
       " role in neopronoun representation both for the tokenizer and the LLM’s context formation, contextual\n",
       " complexity may be introduced when LLMs see these tokens used in ways that do not reflect their\n",
       " dominant contexts Pinter et al. [20,metadata = {'page': 4, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 710b8793-643d-467e-9377-6f6325f85ea6, next_node = 02f38e25-e44c-4522-aa5c-a5e106baab9b, parent_node = None, child_node = []),\n",
       " TextNode(id = 02f38e25-e44c-4522-aa5c-a5e106baab9b,text = 17].\n",
       " 4 Pronoun Tokenization Parity\n",
       " English pronouns serve as building blocks for language acquisition. Termed functional morphemes ,\n",
       " these small, self-contained units of meaning reflect specific English grammatical functions Fortes-\n",
       " cue [2005], Penny Eckert and Ivan A. Sag [2011]. Works have shown that BPE captures varied\n",
       " morphological segments in text Park et al. [2020], Bostrom and Durrett [2020], with BPE subword\n",
       " organization being a reflection of morphological complexity Gutierrez-Vasques et al. [2021]. This\n",
       " unit of meaning seems to be captured for binary pronouns in BPE tokenization, reflected in the fact\n",
       " that it is tokenized as one whole, atomic token. However, unlike binary pronouns, neopronouns like\n",
       " xem are not tokenized as functional whole-word tokens but rather divided into subword units. We\n",
       " posit that this absence of functional preservation hinders the syntactic coherence of neopronouns in\n",
       " LLMs, thereby influencing pronoun consistency. As such, this section offers a mitigation approach\n",
       " informed by these fundamental insights.\n",
       " 4.1 Neopronoun BPE Tokenization\n",
       " In order to improve LLM neopronoun consistency, we introduce pronoun tokenization parity , or\n",
       " PTP, to preserve a token’s functional integrity during BPE tokenization. By aligning this approach\n",
       " with binary pronoun tokenization, we hypothesize that this will improve an LLM’s grammatical\n",
       " representation of neopronouns, thereby improving a model’s ability to use them properly. We produce\n",
       " neopronouns as cohesive linguistic units through the use of special tokens . In LLMs, special tokens\n",
       " may offer functionality such as padding ( [PAD] ) or sentence boundary marking ( [EOS] ), but\n",
       " custom definition of others is possible. We allocate a special token to each neopronoun, allowing it to\n",
       " be represented as a single token, similarly to binary pronouns. We posit that this equivalence will\n",
       " improve the LLM’s capability to capture neopronoun functional morphemes, thereby improving its\n",
       " ability to recognize them as pronouns. Additional details and instructions for reproducing PTP are\n",
       " located in Appendix 1.\n",
       " 4,metadata = {'page': 4, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 21cd7ec8-2686-4fe5-9a63-2fe63eb56be9, next_node = bd95ebdd-a4bf-45fc-9053-ed6bcf05189f, parent_node = None, child_node = []),\n",
       " TextNode(id = bd95ebdd-a4bf-45fc-9053-ed6bcf05189f,text = Token Probability\n",
       " she\n",
       " he\n",
       " xe\n",
       " Evaluation with Constrained DecodingSkyler uses the pronoun xe/xem/xir/xemself. Today,[MASK]went to the bookstore.Pronoun Case Agreement\n",
       " Misgendering\n",
       " Syntactic Error\n",
       " Standard Full Finetune\n",
       " Lexical layer onlyFinetune PLM\n",
       " Tokenize\n",
       " Without Pronoun Token ParityWith Pronoun Token Parityxe \n",
       " x\n",
       " e\n",
       " he\n",
       " she\n",
       " xe\n",
       " he\n",
       " sheMetrics\n",
       " Figure 4: We (1) tokenize neopronouns using PTP for a given LLM, (2) either fully finetune or only\n",
       " finetune the LLM lexical layer with data containing neopronouns, and (3) determine our method’s\n",
       " efficacy in reducing LLM misgendering using constrained decoding approach across 3 metrics.\n",
       " To operationalize PTP, we employ two finetuning paradigms with an open-source LLMs varying\n",
       " in capacity and neopronoun data scarcity. We provide an overview of our framework in Figure 4.\n",
       " The model and data details, along with our performed experiments, are detailed in §5. Formally, we\n",
       " extend the pretrained token embeddings of a transformer-based LLM Eorig\n",
       " 1, Eorig\n",
       " 2, . . . , Eorig\n",
       " n, where n\n",
       " represents the vocabulary size of the original model. We introduce new embeddings EPTPfor each of\n",
       " munique pronouns in the set of neopronoun cases (i.e., pronoun family) S, resulting in an extended\n",
       " vocabulary: {Eorig\n",
       " 1, . . . , Eorig\n",
       " n} ∪ {EPTP\n",
       " 1, . . . , EPTP\n",
       " m}.\n",
       " 4.2 Metrics\n",
       " English pronouns must agree with their subject in gender, case, and number Garner [2016]. To\n",
       " quantify a model’s ability to understand various pronoun forms, we outline three metrics, two of\n",
       " which are derived from these grammar rules and the third is novel metric which we introduce in this\n",
       " paper: pronoun consistency (PC) to assess pronoun-gender agreement; case agreement (CA) to test\n",
       " agreement with pronoun case; and syntactic perturbation error (SE) to assess syntactic robustness\n",
       " to word insertion adversarial attacks which render a sentence grammatically incorrect or change\n",
       " the sentence’s meaning. PC is the most important metric to determine improvement in pronoun-\n",
       " gender agreement, whereas CE & SE provide further insight into sources of error. These metrics are\n",
       " employed in a constrained decoding setting, consistent with Hossain et al. [2023]. Given a masked\n",
       " template, the LLM predicts the most likely pronoun given a pool of pronouns of the same form. We\n",
       " provide further dataset details in Appendix A.5.\n",
       " Pronoun Consistency LetSbe a set of unique pronoun families with |S|pronoun families. Each\n",
       " pronoun family M∈Scontains |M|English pronoun forms. Within a collection of masked templates\n",
       " T,metadata = {'page': 5, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 02f38e25-e44c-4522-aa5c-a5e106baab9b, next_node = a624d74d-1b66-4865-8585-f438455d8805, parent_node = None, child_node = []),\n",
       " TextNode(id = a624d74d-1b66-4865-8585-f438455d8805,text = ,[MASK] is replaced with a pronoun p∈Mfor all M∈S, resulting in the filled template set\n",
       " T∗. In line with Hossain et al. [2023], each template starts with a person’s name and their pronoun\n",
       " declaration (i.e. nominative/accusative/genitive/ reflexive), followed by a sentence containing a\n",
       " [MASK] token which expects a pronoun. For example: Casey uses the pronouns he/him/his/himself.\n",
       " Upon recognizing Casey, the fan asked [MASK] for an autograph. . For a template tconsisting\n",
       " ofmtokens x1, x2, . . . , x m, the token generated at [MASK] , is determined by argmax transition\n",
       " probability from the pronoun pool.\n",
       " ˆyt=argmaxp∈SP(xi=s|x<i) (1)\n",
       " .\n",
       " We denote the set of filled templates as C. Each filled template is then compared to its golden label\n",
       " example c∈C∗, containing the correct pronoun for that template-name-declaration combination.\n",
       " To evaluate pronoun consistency, we compare the model’s chosen pronoun for a template, ˆyt, to the\n",
       " template’s correct pronoun, yc, and then calculate the accuracy over all templates:\n",
       " 1\n",
       " |T∗|X\n",
       " t∈T∗,y∈C∗δ(ˆyt, yc). (2)\n",
       " 5,metadata = {'page': 5, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = bd95ebdd-a4bf-45fc-9053-ed6bcf05189f, next_node = 9f7cdf91-e735-4836-9f74-c9b0f1e3178b, parent_node = None, child_node = []),\n",
       " TextNode(id = 9f7cdf91-e735-4836-9f74-c9b0f1e3178b,text = Case Agreement Evaluating case agreement is essential to assess a model’s pronoun usage profi-\n",
       " ciency. Ideally, an LLM would generate case-agreeing sentences like “She went to the store.” instead\n",
       " of “Hers went to the store.” To evaluate this, we use the same approach as above, instead focusing on\n",
       " assessing expected versus predicted pronoun case for a given pronoun family.\n",
       " We cannot rely on transition probabilities to determine grammatical correctness, as they are condi-\n",
       " tioned only on previous tokens. For example, a sentence like “Casey went to the store for [MASK]\n",
       " mom” can have its mask replaced with “her” or “herself” and still be grammatically correct, as it only\n",
       " considers the previous tokens during inference. Therefore, we acquire the model’s predicted output\n",
       " across all pronoun cases for a given family s∈Q, minimizing its loss (i.e., maximized probability).\n",
       " We then calculate the same accuracy using Eq. (2).\n",
       " argmins∈Q \n",
       " −NX\n",
       " i=1logPθ(xi|x<i)!\n",
       " . (3)\n",
       " Syntactic Perturbation Error Recent work finds that LLM-generated text prompted with neo-\n",
       " pronouns frequently produces ungrammatical text. Ovalle et al. [2023] finds that such text prefixes\n",
       " neopronouns with articles and determiners (e.g., ‘the’, ‘a’, ‘these’). Further quantifying to what\n",
       " extent misgendering correlates with poor grammar highlights potential avenues for amelioration.\n",
       " For instance, Bao and Qiao [2019] find that improving an LLM’s syntax can help binary pronoun\n",
       " resolution in low-resource settings.\n",
       " To assess an LLM’s grammaticality with respect to neopronouns, we introduce a measurement based\n",
       " on adversarial word insertion attacks that mimic the observed ungrammatical behavior found in Ovalle\n",
       " et al. [2023]. Similar to pronoun consistency, we use LLM transition probabilities to evaluate whether\n",
       " and to what extent they tend to use neopronouns in ungrammatical ways, providing insight into the\n",
       " model’s syntactic understanding of them. We use the same templates as before, but now we augment\n",
       " each[MASK] as[DET][MASK] , where[DET] is replaced by singular and plural determiners (e.g.,\n",
       " ‘this’, ‘those’, ‘these’), articles (like ‘the’, ‘a’), or no determiner at all. Example templates are found\n",
       " in subsection A.5. Next, we analyze the LLM’s output by calculating the argmax of the transition\n",
       " probability for all potential substitutions of [DET] . Ideally, an LLM that correctly uses neopronouns\n",
       " will choose a template that does not include a determiner.\n",
       " 5 Experiments\n",
       " In the following sections, we conduct controlled exp,metadata = {'page': 6, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = a624d74d-1b66-4865-8585-f438455d8805, next_node = 64ac7c10-52e3-45d3-8940-2a81f8673904, parent_node = None, child_node = []),\n",
       " TextNode(id = 64ac7c10-52e3-45d3-8940-2a81f8673904,text = eriments to assess the impact of PTP on LLM\n",
       " neopronoun consistency. We compare PTP performance to original BPE tokenization across different\n",
       " resource settings, shedding light on the role tokenizers play in LLM pronoun consistency gaps.\n",
       " Additionally, we explore resource-efficient mitigation techniques and evaluate the scalability of our\n",
       " methodology, providing insights into scenarios with the most substantial improvements.\n",
       " We focus on using PTP for the neopronoun family xe, for several reasons. First, xeranks among the\n",
       " most widely adopted non-binary pronouns Gender Census [2023]. Second, it is well-documented\n",
       " that non-binary pronouns exhibit a diverse range of linguistic variations, spanning from closed to\n",
       " open word class forms Miltersen [2016], Lauscher et al. [2022]. This diversity requires a nuanced yet\n",
       " flexible approach. By focusing on the xepronoun family, we showcase the effectiveness of PTP while\n",
       " providing a generalizable framework for researchers to build off for studying non-binary pronouns\n",
       " within their respective linguistic contexts.\n",
       " Models We conduct our experiments using the Pythia model suite.3We choose this framework as\n",
       " it parallels state-of-the-art architecture; Pythia models all consist of a GPT-Neo-X architecture, an\n",
       " open-source alternative to GPT-3 models. Notably, it is based on a BPE tokenizer Biderman et al.\n",
       " [2023] and trained on a commonly accessible dataset, the PILE. Furthermore, as these models vary\n",
       " from 70M to 12B parameters, they provide an ideal environment for investigating LLM knowledge\n",
       " development with PTP across model capacity.\n",
       " Datasets Due to the limited availability of textual data containing neopronouns, we make use of\n",
       " the Wikibios dataset,4which consists of narratives about real individuals. To mitigate this scarcity,\n",
       " 3https://github.com/EleutherAI/pythia\n",
       " 4https://huggingface.co/datasets/wiki_bio\n",
       " 6,metadata = {'page': 6, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 9f7cdf91-e735-4836-9f74-c9b0f1e3178b, next_node = 5961b3c6-4b27-4e13-91b8-2a2b8f578be2, parent_node = None, child_node = []),\n",
       " TextNode(id = 5961b3c6-4b27-4e13-91b8-2a2b8f578be2,text = ModelPronoun Consistency ( ↑) Case Agreement ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.968 0.716 0.007 0.677 0.607 0.200 0.238 0.169 0.850\n",
       " TOrig+MFull 0.896 0.861 0.145 0.685 0.597 0.200 0.239 0.168 0.895\n",
       " TOrig+MLex 0.865 0.729 0.168 0.617 0.572 0.200 0.290 0.232 0.654\n",
       " TPTP+MFull 0.948 0.835 0.378 0.695 0.611 0.294 0.278 0.210 0.270\n",
       " TPTP+MLex 0.850 0.722 0.536 0.632 0.591 0.325 0.258 0.218 0.348\n",
       " Table 1: 70M-parameter model results at 10% data resource level. TOrig= withoutPTP,TPTP= with\n",
       " PTP,MFull= full finetuning, MLex= only lexical finetuning. MBaseline = original model, no finetuning.\n",
       " 10 20 30 40 50\n",
       " Data Split0.00.20.40.60.81.0Pronoun Consistency\n",
       " Pronoun Family: he\n",
       " TMono+MBaseline\n",
       " TMono+MFull\n",
       " TNeoR+MFull\n",
       " 10 20 30 40 50\n",
       " Data Split\n",
       " Pronoun Family: she\n",
       " 10 20 30 40 50\n",
       " Data Split\n",
       " Pronoun Family: xe\n",
       " Figure 5: 70M-parameter model with full finetuning across data resource sizes. Plot shows using\n",
       " PTP most effective in maintaining pronoun consistency for pronoun family xe.\n",
       " we employ a counterfactual data augmentation technique that replaces a variable fraction of binary\n",
       " pronouns with their neopronoun equivalents. We do this for two primary reasons. Firstly, individuals\n",
       " who use neopronouns often have historical associations with binary pronouns. Recognizing this and\n",
       " considering the substantial shortage of neopronoun representation in pretraining corpora, we aim\n",
       " to incorporate narratives that resonate with a broader spectrum of individuals Talat and Lauscher\n",
       " [2022]. This approach enables LLMs to learn neopronoun usage within more comprehensive, diverse,\n",
       " and real-world contexts. Secondly, the dataset is readily available via HuggingFace, allowing for\n",
       " ease in reproducibility. We filtered the Wikibios dataset, comprised of 728,321 English biographical\n",
       " texts from Wikipedia, to retain texts containing binary pronouns, resulting in 462,345 examples. We\n",
       " replaced each binary pronoun with its corresponding case for xe, incorporating correct possessive\n",
       " forms using the spaCy part-of-speech tagger [Honnibal et al., 2020].\n",
       " To evaluate PTP, we conducted experiments fine-tuning models on this augmented data with varying\n",
       " amounts of neopronouns, representing different resource levels: 10%, 20%, 30%, 40%, and 50%.\n",
       " In the 50% setting, the dataset is evenly split between xeand binary pronouns. No biography text\n",
       " appears more than once in the dataset splits. For evaluation, we utilized the MISGENDERED dataset\n",
       " by Hossain et al. [2023], containing added templates and,metadata = {'page': 7, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 64ac7c10-52e3-45d3-8940-2a81f8673904, next_node = 116f8f77-f1e2-4ac8-9ce9-77c2351457f4, parent_node = None, child_node = []),\n",
       " TextNode(id = 116f8f77-f1e2-4ac8-9ce9-77c2351457f4,text =  names from TANGO [Ovalle et al., 2023]\n",
       " (more details in §A.5), resulting in 93,600 templates for evaluation across our defined metrics.\n",
       " 5.1 Experiment 1: How helpful is Pronoun Token Parity for reducing misgendering in LLMs?\n",
       " We assess PTP’s impact on reducing misgendering for a compact 70M parameter Pythia model.\n",
       " We prepare two versions for fine-tuning: one without PTP (original tokenizer, TOrig) and one with\n",
       " PTP ( TPTP). PTP embeddings for TPTPare initialized with a random Gaussian ( µ=0 and σ=0.02).\n",
       " MFullencompasses all models fine-tuned with standard full finetuning, and MBaseline represents the\n",
       " HuggingFace baseline checkpoint, utilizing the non-PTP tokenizer TOrig. Fine-tuning is done for five\n",
       " epochs with a batch size of 128, a learning rate of 10−4, and early stopping based on cross-entropy\n",
       " loss in the validation set with a patience of 2. To expedite model training and inference, all models\n",
       " undergo fine-tuning using FP16 mixed precision and two gradient accumulation steps. We provide\n",
       " further details on our setup in §A.4.\n",
       " Results As results in Table 1 show, both TPTP+MFullandTOrig+MFulldemonstrate an improved\n",
       " neopronoun consistency over the baseline 70M Pythia model. This improvement is expected, con-\n",
       " sidering their increased exposure to neopronouns during fine-tuning. Interestingly, models using a\n",
       " tokenizer with PTP consistently outperformed those without PTP, as shown in Figure 5. Improvement\n",
       " over this baseline is observed across data resource levels, especially at lower resource levels, where\n",
       " TPTP+MFullmore than doubles the consistency improvements from TOrig+MFull(37.8% vs. 14.5%).\n",
       " Binary pronoun consistency remains stable with PTP, with TPTP+MFulleven enhancing shepronoun\n",
       " consistency. Our findings indicate that a significant portion of neopronoun consistency disparities can\n",
       " 7,metadata = {'page': 7, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 5961b3c6-4b27-4e13-91b8-2a2b8f578be2, next_node = 964d3657-cc35-40bc-a8d8-bc216eff532d, parent_node = None, child_node = []),\n",
       " TextNode(id = 964d3657-cc35-40bc-a8d8-bc216eff532d,text = be attributed to OOV tokenization due to neopronoun scarcity, motivating further investigation into\n",
       " potential enhancements to tokenization.\n",
       " 5.2 Experiment 2: Can a more resource-efficient approach to PTP still reduce LLM\n",
       " misgendering?\n",
       " Full finetuning can be resource-intensive, posing challenges for machine learning researchers due\n",
       " to the associated time and compute costs. Since Pythia has already learned English syntax and\n",
       " binary pronouns from its pretraining, we hypothesize that full finetuning may not be necessary\n",
       " to learn new neopronouns following English grammar rules. Inspired by cross-lingual transfer\n",
       " techniques Artetxe et al. [2019b], de Vries and Nissim [2020], we experiment with finetuning only\n",
       " Pythia’s lexical embedding layer, leaving transformer weights unchanged. Unlike Artetxe et al.\n",
       " [2019b], we avoid training the transformer weights after freezing lexical embeddings since the new\n",
       " tokens already conform to English grammar and syntax, eliminating the need for the transformer\n",
       " to adapt to a different language. Additionally, differing from the approach by de Vries and Nissim\n",
       " [2020], we avoid resetting the entire lexical embedding layer to retain the prelearned English grammar\n",
       " dependencies. We compare models that only had their lexical layers finetuned, MLex, with a baseline\n",
       " where a non-PTP model’s lexical layer is finetuned. We follow the same setup as before but increase\n",
       " the learning rate to 10−3to encourage more rapid adaptation to the new vocabulary.\n",
       " Results As shown in Table 1, employing PTP with lexical finetuning outperformed standard full\n",
       " finetuning with the original BPE tokenizer (53.6% vs. 37.8%). TPTP+MLexalso outperformed TOrig\n",
       " +MLex(53.6% vs 16.8%), further supporting that benefits come from the PTP, rather than the lexical\n",
       " finetuning alone. Notably, MLexconsistently outperformed finetuning across various data resource\n",
       " levels (see Appendix Table 7a). Given identical context-dependent structures between pronouns,\n",
       " these results demonstrate pronoun adaptation via only learning a new context-independent lexical\n",
       " layer, rather than learning both lexical tokens and how to use them in-context.\n",
       " Both case agreement and syntactic error showed moderate to very strong relationships with pronoun\n",
       " consistency (CA Spearman ρ=.841, SE Spearman ρ=−.644, ).TPTP+MLexresulted in the\n",
       " highest case agreement (32.5%), as shown in Table 1. Interestingly, even when trained on more\n",
       " data and using both training regimes, syntactic errors did not decrease as signific,metadata = {'page': 8, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 116f8f77-f1e2-4ac8-9ce9-77c2351457f4, next_node = d0fd4d79-1f48-4aed-9f7d-a53fa3fc6f24, parent_node = None, child_node = []),\n",
       " TextNode(id = d0fd4d79-1f48-4aed-9f7d-a53fa3fc6f24,text = antly with TOrig\n",
       " as with TPTP, suggesting that PTP helps an LLM improve its grammatical usage of neopronouns.\n",
       " Additionally, MLexfor 70M resulted in an 18.8% reduction in training time compared to MFull,\n",
       " presenting a more resource-efficient and eco-friendly alternative to standard full fine-tuning (more\n",
       " results in Appendix Table 4). Regarding binary pronouns, the effects of lexical fine-tuning varied for\n",
       " models of this size, with some showing temporary increases in consistency and others improving\n",
       " with full fine-tuning. This may be related to nuances in lexically finetuning existing versus new\n",
       " pronouns. Binary pronouns, having already converged in latent lexical space, may subtly shift during\n",
       " finetuning. However, neopronouns are not initialized from an initial space of semantic relevance,\n",
       " thereby requiring the LLM to learn them from scratch. Consequently, while the LLM learns these new\n",
       " neopronouns, the already trained binary pronouns may be impacted. These trends extended to case\n",
       " agreement and syntactic error, though the variations in binary pronoun values were not statistically\n",
       " different from lexical finetuning without PTP.\n",
       " 5.3 Experiment 3: Does a bigger LLM always mean better pronoun consistency?\n",
       " In this experiment, our questions consider scale: (1) Does our methodology work for model sizes\n",
       " beyond 70M? (2) Does an increase in model capacity always mean improved pronoun consistency?\n",
       " And if not, (3) at what point do we maximize improvements with PTP? To answer these, we run\n",
       " experiments encompassing the previous experiments across 70M, 160M, 410M, and 1.4B parameter\n",
       " models. Each model is finetuned and evaluated in the same way as prior experiments, using only a\n",
       " 10% resource split in order to best reflect realistic low-resource scenarios with neopronouns.\n",
       " Results Our results highlight the effectiveness of lexical training for PTP across different model\n",
       " sizes, with smaller models achieving performance boosts comparable to models more than twice their\n",
       " size. Figure 6 demonstrates that, across all model sizes, TPTP+MLexconsistently improved pronoun\n",
       " consistency over TOrigacross finetuning regimes. Across both PTP and lexical finetuning respectively,\n",
       " we find these models maintain high quality generations, as shown in Table 2. We consistently\n",
       " 8,metadata = {'page': 8, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 964d3657-cc35-40bc-a8d8-bc216eff532d, next_node = bae8ab04-4506-45f9-9352-65761b63ba06, parent_node = None, child_node = []),\n",
       " TextNode(id = bae8ab04-4506-45f9-9352-65761b63ba06,text = Table 2: Generations from Pythia-410M from the prompt: Skyler’s pronouns are xe/xem/xir/xirself.\n",
       " Generations Text\n",
       " TOrig+MBaseline . . . Xe needs to go to the bookstore soon because it’s not in the store. Xir is the same as Xirself.\n",
       " TOrig+MFull . . . Xe needs to go to the bookstore soon because xe’s too busy to be on the air. Xe’s also a good friend of xir.\n",
       " TPTP+MFull . . . Xe needs to go to the bookstore soon because xe’s going to be writing a book about how to say xir name. Xe also has a\n",
       " book in the works called \"the art of being a writer.\n",
       " TPTP+MLex . . . Xe is a huge fan of the book “the secret life of the apes” by john mccarthy.\n",
       " 70m 160m 410m 1.4b0.00.20.40.60.81.0Pronoun Consistency\n",
       " Pronoun Family: he\n",
       " TOrig+MBaseline\n",
       " TOrig+MFull\n",
       " TOrig+MLex\n",
       " TPTP+MFull\n",
       " TPTP+MLex\n",
       " 70m 160m 410m 1.4b\n",
       " Pronoun Family: she\n",
       " 70m 160m 410m 1.4b\n",
       " Pronoun Family: xe\n",
       " Figure 6: Misgendering Rates across model sizes. Highest consistency for neopronouns seen in\n",
       " models using PTP.\n",
       " observed case agreement improvements for certain binary pronouns when MLexwas added (see\n",
       " Appendix Table 8). Between MLexandMFull, PTP was most effective with lexical finetuning in\n",
       " models smaller than 410M parameters, while finetuning proved more beneficial for larger models.\n",
       " The highest pronoun consistency for xewas achieved using PTP with a 410M parameter model (58%)\n",
       " using full finetuning, followed by a 160M parameter model using only lexical finetuning (55%).\n",
       " While the 410M model did show lower syntactic error (27%, vs 35% for 160M) and a higher average\n",
       " case agreement (48% vs. 40%), these results indicate that high levels of pronoun consistency seen in\n",
       " large models can also be attained in smaller models given appropriate mitigation strategies.\n",
       " 6 Conclusion\n",
       " In this work, we demonstrate how LLM misgendering is influenced by low-resource BPE tokenization.\n",
       " We find that We demonstrate this empirically, finding that LLM misgendering closely linked its\n",
       " inability to adhere to neopronoun morphosyntax. With this knowledge, we propose a mitigation\n",
       " procedure called pronoun tokenization parity , designed to preserve neopronoun functional structure\n",
       " through special tokens. We find that LLMs finetuned with PTP reduces neopronoun misgendering\n",
       " over traditional finetuning settings without PTP. Likewise, exploiting pre-existing English gram-\n",
       " matical knowledge with PTP also achieves effective mitigation in a cost-effective manner. As BPE\n",
       " tokenization is just one of many subword tokenization algorithms, our work lays groundwork for,metadata = {'page': 9, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = d0fd4d79-1f48-4aed-9f7d-a53fa3fc6f24, next_node = 7b21d75e-44da-4a48-8fe9-c95be989bb84, parent_node = None, child_node = []),\n",
       " TextNode(id = 7b21d75e-44da-4a48-8fe9-c95be989bb84,text = \n",
       " exploring misgendering in LLMs across these schemes. Future work may also study misgendering\n",
       " due to tokenization within a multilingual setting.\n",
       " Limitations and Broader Impacts\n",
       " As neopronouns continue to surface and be adopted, we highlight the importance of considering how\n",
       " each pronoun family operates within its own language. Therefore, we show this as an end-to-end\n",
       " example for one pronoun family, xewith respect to English. Future work should also consider the how\n",
       " pronoun family operates within the LLM as well. Furthermore, adding other metrics from existing\n",
       " bias benchmarks may complement our study. We mostly rely on quantitative metrics grounded in\n",
       " English grammar rules to assess the quality of mitigations. While an in-depth human evaluation may\n",
       " also supplement this work, we qualitatively evaluate resulting text generations from our approaches\n",
       " (please see Appendix Table 2).\n",
       " We also emphasize the importance of transparent stakeholder discourse in selecting an approach that\n",
       " balances pronoun consistency, error rates, and case agreement. For instance, if choosing to address\n",
       " historical disparities for minority groups, stakeholders may choose to prioritize their improvement\n",
       " while specifying a error tolerance for dominant groups rather than solely aiming for equal or improved\n",
       " performance across majority groups.\n",
       " 9,metadata = {'page': 9, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = bae8ab04-4506-45f9-9352-65761b63ba06, next_node = 709ac864-3482-408d-bf11-281be2729444, parent_node = None, child_node = []),\n",
       " TextNode(id = 709ac864-3482-408d-bf11-281be2729444,text = References\n",
       " Ali Araabi, Christof Monz, and Vlad Niculae. How effective is byte pair encoding for out-of-\n",
       " vocabulary words in neural machine translation? In Conference of the Association for Ma-\n",
       " chine Translation in the Americas , 2022. URL https://api.semanticscholar.org/\n",
       " CorpusID:251468147 .\n",
       " Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Bilingual lexicon induction through un-\n",
       " supervised machine translation. ArXiv , abs/1907.10761, 2019a. URL https://api.\n",
       " semanticscholar.org/CorpusID:196187034 .\n",
       " Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolin-\n",
       " gual representations. In Annual Meeting of the Association for Computational Linguistics , 2019b.\n",
       " URLhttps://api.semanticscholar.org/CorpusID:204901567 .\n",
       " Xingce Bao and Qianqian Qiao. Transfer learning from pre-trained bert for pronoun resolution. In\n",
       " Proceedings of the first workshop on gender bias in natural language processing , pages 82–88,\n",
       " 2019.\n",
       " Stella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O’Brien,\n",
       " Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\n",
       " Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing\n",
       " large language models across training and scaling. ArXiv , abs/2304.01373, 2023. URL https:\n",
       " //api.semanticscholar.org/CorpusID:257921893 .\n",
       " Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. In\n",
       " Findings , 2020. URL https://api.semanticscholar.org/CorpusID:215416175 .\n",
       " Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n",
       " Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n",
       " few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\n",
       " František Da ˇrena and Martin Suss. Quality of word vectors and its impact on named entity recognition\n",
       " in czech. European Journal of Business Science and Technology , 2020. URL https://api.\n",
       " semanticscholar.org/CorpusID:231686297 .\n",
       " Wietse de Vries and Malvina Nissim. As good as new. how to successfully recycle english gpt-\n",
       " 2 to make models for other languages. ArXiv , abs/2012.05628, 2020. URL https://api.\n",
       " semanticscholar.org/CorpusID:228083868 .\n",
       " Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff Phillips, and Kai-Wei\n",
       " Chang. Harms of gender exclusivity and challenges in non-binary representation in language\n",
       " technologies. In Proceedings of the 2021 Conference on Empiric,metadata = {'page': 10, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 7b21d75e-44da-4a48-8fe9-c95be989bb84, next_node = df2259ff-9860-44f5-b67c-ab5e6dc6f386, parent_node = None, child_node = []),\n",
       " TextNode(id = df2259ff-9860-44f5-b67c-ab5e6dc6f386,text = al Methods in Natural Language\n",
       " Processing , pages 1968–1994, Online and Punta Cana, Dominican Republic, November 2021.\n",
       " Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.150. URL https:\n",
       " //aclanthology.org/2021.emnlp-main.150 .\n",
       " Miguel Domingo, Mercedes García-Martínez, Alexandre Helle, Francisco Casacuberta, and Manuel\n",
       " Herranz. How much does tokenization affect neural machine translation? In Conference\n",
       " on Intelligent Text Processing and Computational Linguistics , 2018. URL https://api.\n",
       " semanticscholar.org/CorpusID:56517054 .\n",
       " Virginia K. Felkner, Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May. Wino-\n",
       " queer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models.\n",
       " ArXiv , abs/2306.15087, 2023. URL https://api.semanticscholar.org/CorpusID:\n",
       " 259262064 .\n",
       " M.D. Fortescue. Historical Linguistics 2003: Selected Papers from the 16th International Conference\n",
       " on Historical Linguistics, Copenhagen, 11-15 August 2003 . Amsterdam Studies in the Theory and\n",
       " History of Linguistic Science: 4. J. Benjamins Pub., 2005. ISBN 978-1-58811-586-7.\n",
       " B.A. Garner. The Chicago Guide to Grammar, Usage, and Punctuation . Chicago Guides to Writing,\n",
       " Editing, and Publishing. University of Chicago Press, 2016. ISBN 978-0-226-19129-4.\n",
       " 10,metadata = {'page': 10, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 709ac864-3482-408d-bf11-281be2729444, next_node = d4a4e317-6194-4ccd-822a-1abcf4a0e4d1, parent_node = None, child_node = []),\n",
       " TextNode(id = d4a4e317-6194-4ccd-822a-1abcf4a0e4d1,text = Gender Census. 2023 gender census, 2023. URL https://www.gendercensus.com/\n",
       " results/2023-worldwide/#pronouns . Accessed: September 14, 2023.\n",
       " Ximena Gutierrez-Vasques, Christian Bentz, O. A. Sozinova, and Tanja Samardvzic. From characters\n",
       " to words: the turning point of bpe merges. In Conference of the European Chapter of the\n",
       " Association for Computational Linguistics , 2021. URL https://api.semanticscholar.\n",
       " org/CorpusID:233189533 .\n",
       " Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-\n",
       " strength Natural Language Processing in Python. 2020. doi: 10.5281/zenodo.1212303.\n",
       " Tamanna Hossain, Sunipa Dev, and Sameer Singh. Misgendered: Limits of large language models in\n",
       " understanding pronouns. arXiv preprint arXiv:2306.03950 , 2023.\n",
       " Matthias Huck, Viktor Hangya, and Alexander M. Fraser. Better oov translation with bilingual\n",
       " terminology mining. In Annual Meeting of the Association for Computational Linguistics , 2019.\n",
       " URLhttps://api.semanticscholar.org/CorpusID:196193105 .\n",
       " Anne Lauscher, Archie Crowley, and Dirk Hovy. Welcome to the modern world of pronouns:\n",
       " Identity-inclusive natural language processing beyond gender. arXiv preprint arXiv:2202.11923 ,\n",
       " 2022.\n",
       " Ninareh Mehrabi, Thamme Gowda, Fred Morstatter, Nanyun Peng, and A. G. Galstyan. Man is to\n",
       " person as woman is to location: Measuring gender bias in named entity recognition. Proceedings\n",
       " of the 31st ACM Conference on Hypertext and Social Media , 2019. URL https://api.\n",
       " semanticscholar.org/CorpusID:204851964 .\n",
       " Ehm Hjorth Miltersen. Nounself pronouns: 3rd person personal pronouns as identity expression.\n",
       " Journal of Language Works-Sprogvidenskabeligt Studentertidsskrift , 1(1):37–62, 2016.\n",
       " Debora Nozza, Federico Bianchi, Anne Lauscher, Dirk Hovy, et al. Measuring harmful sentence\n",
       " completion in language models for lgbtqia+ individuals. In Proceedings of the Second Workshop\n",
       " on Language Technology for Equality, Diversity and Inclusion . Association for Computational\n",
       " Linguistics, 2022.\n",
       " Organizers of QueerInAI, Nathaniel Dennler, Anaelia Ovalle, Ashwin Singh, Luca Soldaini, Arjun\n",
       " Subramonian, Huy Tu, William Agnew, Avijit Ghosh, Kyra Yee, Irene Font Peradejordi, Zeerak\n",
       " Talat, Mayra Russo, and Jessica de Jesus de Pinho Pinhal. Bound by the bounty: Collaboratively\n",
       " shaping evaluation processes for queer ai harms. Proceedings of the 2023 AAAI/ACM Conference on\n",
       " AI, Ethics, and Society , 2023. URL https://api.semanticscholar.org/CorpusID:\n",
       " 259991807 .\n",
       " Anaelia Ovalle, Palash Goyal, Jwala Dhamala, Zac,metadata = {'page': 11, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = df2259ff-9860-44f5-b67c-ab5e6dc6f386, next_node = a2b1c82a-a074-4de7-833d-cb1d4902907f, parent_node = None, child_node = []),\n",
       " TextNode(id = a2b1c82a-a074-4de7-833d-cb1d4902907f,text = hary Jaggers, Kai-Wei Chang, Aram Galstyan,\n",
       " Richard Zemel, and Rahul Gupta. “i’m fully who i am”: Towards centering transgender and\n",
       " non-binary voices to measure biases in open language generation. In Proceedings of the 2023\n",
       " ACM Conference on Fairness, Accountability, and Transparency , pages 1246–1266, 2023.\n",
       " Hyunji Hayley Park, Katherine J. Zhang, Coleman Haley, Kenneth Steimel, Han Liu, and Lane\n",
       " Schwartz. Morphology matters: A multilingual language modeling analysis. Transactions\n",
       " of the Association for Computational Linguistics , 9:261–276, 2020. URL https://api.\n",
       " semanticscholar.org/CorpusID:228375396 .\n",
       " Penny Eckert and Ivan A. Sag. Morphology, 2011. URL https://web.stanford.edu/\n",
       " class/linguist1/Slides/morph2-slides.pdf . Accessed: 2023.\n",
       " Yuval Pinter, Robert Guthrie, and Jacob Eisenstein. Mimicking word embeddings using subword rnns.\n",
       " ArXiv , abs/1707.06961, 2017. URL https://api.semanticscholar.org/CorpusID:\n",
       " 10361075 .\n",
       " Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n",
       " models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n",
       " Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\n",
       " coreference resolution. arXiv preprint arXiv:1804.09301 , 2018.\n",
       " 11,metadata = {'page': 11, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = d4a4e317-6194-4ccd-822a-1abcf4a0e4d1, next_node = 8d9c1ab0-ed25-43d6-895c-d59e3b844723, parent_node = None, child_node = []),\n",
       " TextNode(id = 8d9c1ab0-ed25-43d6-895c-d59e3b844723,text = Phillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian Ruder, and Iryna Gurevych. How good is your\n",
       " tokenizer? on the monolingual performance of multilingual language models. In Proceedings of the\n",
       " 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\n",
       " Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 3118–3135,\n",
       " Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.\n",
       " 243. URLhttps://aclanthology.org/2021.acl-long.243 .\n",
       " Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation mod-\n",
       " els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association\n",
       " for Computational Linguistics (Volume 1: Long Papers) , pages 86–96, Berlin, Germany, Au-\n",
       " gust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL\n",
       " https://aclanthology.org/P16-1009 .\n",
       " Karolina Stanczak and Isabelle Augenstein. A survey on gender bias in natural language processing.\n",
       " arXiv preprint arXiv:2112.14168 , 2021.\n",
       " Gabriel Stanovsky, Noah A Smith, and Luke Zettlemoyer. Evaluating gender bias in machine\n",
       " translation. arXiv preprint arXiv:1906.00591 , 2019.\n",
       " Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza,\n",
       " Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. Mitigating gender bias in natural\n",
       " language processing: Literature review. arXiv preprint arXiv:1906.08976 , 2019.\n",
       " Zeerak Talat and Anne Lauscher. Back to the future: On potential histories in nlp.\n",
       " ArXiv , abs/2210.06245, 2022. URL https://api.semanticscholar.org/CorpusID:\n",
       " 252846634 .\n",
       " Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\n",
       " Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\n",
       " efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n",
       " Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, and Dong Yu. Improving pre-trained multilin-\n",
       " gual model with vocabulary expansion. ArXiv , abs/1909.12440, 2019. URL https://api.\n",
       " semanticscholar.org/CorpusID:203586495 .\n",
       " Xiao Wang, Shihan Dou, Li Xiong, Yicheng Zou, Qi Zhang, Tao Gui, Liang Qiao, Zhanzhan Cheng,\n",
       " and Xuanjing Huang. Miner: Improving out-of-vocabulary named entity recognition from an infor-\n",
       " mation theoretic perspective. In Annual Meeting of the Association for Computational Linguistics ,\n",
       " 2022. URL https://api.semanticscholar.org/CorpusID:248085423 .\n",
       " Alfan Farizki Wicaksono and Ayu Purwariant,metadata = {'page': 12, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = a2b1c82a-a074-4de7-833d-cb1d4902907f, next_node = ab46035a-0db5-4fe2-a91d-d365eb886caa, parent_node = None, child_node = []),\n",
       " TextNode(id = ab46035a-0db5-4fe2-a91d-d365eb886caa,text = i. Hmm based part-of-speech tagger f or bahasa\n",
       " indonesia. 2010. URL https://api.semanticscholar.org/CorpusID:63251684 .\n",
       " Shaked Yehezkel and Yuval Pinter. Incorporating context into subword vocabularies. In Conference\n",
       " of the European Chapter of the Association for Computational Linguistics , 2022. URL https:\n",
       " //api.semanticscholar.org/CorpusID:252872957 .\n",
       " Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\n",
       " coreference resolution: Evaluation and debiasing methods. In North American Chapter of the\n",
       " Association for Computational Linguistics , 2018. URL https://api.semanticscholar.\n",
       " org/CorpusID:4952494 .\n",
       " 12,metadata = {'page': 12, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 8d9c1ab0-ed25-43d6-895c-d59e3b844723, next_node = a60f4677-348b-4105-806f-94d49c03b32c, parent_node = None, child_node = []),\n",
       " TextNode(id = a60f4677-348b-4105-806f-94d49c03b32c,text = A Appendix\n",
       " A.1 Details on How to Reproduce Pronoun Tokenization Parity (PTP)\n",
       " We provide details on how to reproduce PTP in Algorithm 1.\n",
       " Algorithm 1 Pronoun Tokenization Parity (PTP)\n",
       " 1:Input 1 : LLM model\n",
       " 2:Input 2 : LLM model’s BPE tokenizer\n",
       " 3:Input 3 : Defined list of neopronouns for PTP\n",
       " 4:Input 4 : Dataset augmented with neopronouns\n",
       " 5:Method : Add special tokens for each neopronoun. Be sure to explicitly add ’ ˙G’ to the beginning\n",
       " of each token to indicate that it is a full, non-subword token space before the word, otherwise\n",
       " this will lead to incorrect model behavior, since a lack of ’ ˙G’ in BPE tokenization indicates a\n",
       " subword token.\n",
       " 6:Check : Check the tokenizer is working properly by checking the tokenized neopronoun, ensuring\n",
       " that you see ’ ˙G’ in its token. For example, tokenizing xeshould result in [’ ˙Gxe’] not [’ ˙G’, xe’].\n",
       " The latter will cause the LLM to incorrectly associate a space character with a neopronoun. This\n",
       " can be tested by checking next word transition probabilities from the space character.\n",
       " 7:Resize the LLM token embeddings to match vocabulary of tokenizer. Here is example code to do\n",
       " this with a model and tokenizer from HuggingFace Transformers Package5.\n",
       " #declare neopronoun tokens\n",
       " arr_tokens = [\n",
       " ’˙Gxe’, ’˙GXe’,\n",
       " ’˙Gxem’, ’ ˙GXem’,\n",
       " ’˙Gxir’, ’ ˙GXir’,\n",
       " ’˙Gxirs’, ’ ˙GXirs’\n",
       " ]\n",
       " # add new tokens to the tokenizer, t\n",
       " token_dict = {\n",
       " ’additional_special_tokens’: arr_tokens\n",
       " }\n",
       " t.add_special_tokens(token_dict)\n",
       " # update model, m, accordingly\n",
       " m.resize_token_embeddings(len(tokenizer))\n",
       " 8:ifLexical Finetuning then\n",
       " 9: Freeze all parameters besides the word token embeddings. Then proceed to finetune this\n",
       " lexical layer.\n",
       " 10:else\n",
       " 11: Proceed with standard full finetuning\n",
       " 12:Return Finetuned model, new PTP tokenizer\n",
       " 13:Evaluate using extended MISGENDERED framework\n",
       " A.2 Measuring Fertility\n",
       " We measure “fertility” as defined by Rust et al. [2021]. We define binary pronouns as pronouns\n",
       " across all cases for heandshe. We define the neopronoun group as all pronoun cases for common\n",
       " neopronouns xe,ey, and fae.610k most common words are determined by n-gram frequency analysis\n",
       " of the Google’s Trillion Word Corpus.7\n",
       " 6https://nonbinary.wiki/wiki/English_neutral_pronouns\n",
       " 7https://github.com/first20hours/google-10000-english/blob/master/\n",
       " google-10000-english-usa-no-swears-short.txt\n",
       " 13,metadata = {'page': 13, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = ab46035a-0db5-4fe2-a91d-d365eb886caa, next_node = 668eef49-c88c-4151-b9da-4cadca775187, parent_node = None, child_node = []),\n",
       " TextNode(id = 668eef49-c88c-4151-b9da-4cadca775187,text = A.3 Embedding Initialization\n",
       " Upon adding a new token and creating a new EPTP, embeddings are set to default random initialization\n",
       " behavior in an LLM. Being that neopronouns and binary pronouns follow the same grammar rules\n",
       " in English, we also investigate leveraging existing grammatical knowledge learned by the LLM to\n",
       " help bootstrap the model’s ability to learn to use neopronouns better. Establishing a direct mapping\n",
       " between binary and neopronouns across their various forms, we average the neopronoun embedding\n",
       " with its corresponding binary pronoun embedding for each case. This approach resembles the use of\n",
       " a bilingual lexicon to facilitate vocabulary alignment Artetxe et al. [2019a].\n",
       " We adopt the method of taking the mean across binary pronouns for two key reasons: to leverage\n",
       " the LLM’s syntactic knowledge related to singular pronouns used similarly to xein sentences and to\n",
       " accommodate individuals who use neopronouns and may have historical associations with binary\n",
       " pronouns. This is denoted in the tables from Section A.6 as PTP-B. For future work, we encourage\n",
       " further exploration of methods to bootstrap these embeddings.\n",
       " A.4 Model Finetuning Details\n",
       " A.4.1 Experiment 1 - Full Finetuning\n",
       " We use the deduped versions of Pythia, which trained on the Pile after the dataset had been globally\n",
       " deduplicated. We confirm that our research is in line with Pythia’s intended use: Given their Apache\n",
       " 2.0 license, we may finetune or adapt these models.\n",
       " Before tokenization, text is chunked with a 256 window size, resulting in 386,267 rows before\n",
       " any neopronoun augmentation. We conduct fine-tuning with an 80/10/10 train, validation, and test\n",
       " split. Each model adheres to Pythia suite configurations, including an embedding size of 512 and a\n",
       " vocabulary size of 50,284 (50,277 without PTP). Fine-tuning is done for five epochs with a batch size\n",
       " of 128, a learning rate of 10−4, and early stopping based on cross-entropy loss in the validation set\n",
       " with a patience of 2. To expedite model training, all models undergo fine-tuning using FP16 mixed\n",
       " precision and 2 gradient accumulation steps.\n",
       " A.4.2 Experiment 2 - Lexical Training\n",
       " We follow the setup from the previous experiment, but only slightly increase the learning rate to 1 x\n",
       " 103in order to encourage more rapid adaptation to the new vocabulary.\n",
       " A.4.3 Hardware Setup\n",
       " We perform all our experiments with 8 NVIDIA A100s with 40 GiB vRAM.\n",
       " Table 3: Average GPU Hours For Full Finetuning\n",
       " Model Size Hours\n",
       " 70M 0.65\n",
       " 160M 0.74\n",
       " 410M 1.2\n",
       " 1.4B 1.7\n",
       " Tabl,metadata = {'page': 14, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = a60f4677-348b-4105-806f-94d49c03b32c, next_node = 4010952d-b10c-4211-89be-d8798dd0dda8, parent_node = None, child_node = []),\n",
       " TextNode(id = 4010952d-b10c-4211-89be-d8798dd0dda8,text = e 4: ∆Training runtime upon switching to lexical finetuning.\n",
       " Model Size Training Time Reduction (%)\n",
       " 70M 18.8\n",
       " 160M 21.1\n",
       " 410M 16.5\n",
       " 1.4B 21.5\n",
       " 14,metadata = {'page': 14, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 668eef49-c88c-4151-b9da-4cadca775187, next_node = 2b17bb07-6839-49a1-81da-1590b3f0ecb1, parent_node = None, child_node = []),\n",
       " TextNode(id = 2b17bb07-6839-49a1-81da-1590b3f0ecb1,text = Table 5: Model Parameters (P), Available on HuggingFace.\n",
       " Model Size # P # Non-Embedding P\n",
       " 70M 70,426,624 18,915,328\n",
       " 160M 162,322,944 85,056,000\n",
       " 410M 405,334,016 302,311,424\n",
       " 1.4B 1,414,647,808 1,208,602,624\n",
       " A.5 Templates additions to MISGENDERED\n",
       " To mimic real world pronoun declarations, each declaration is started with nominative, accusative,\n",
       " pronominal possessive, and reflexive pronouns.\n",
       " Table 6 reflects selected additions from the TANGO dataset.Det represents the determiner position\n",
       " one may replace with ones like the, a, these, those .Gen-dep, Gen-indep, reflex, nom\n",
       " are all pronoun cases.\n",
       " Table 6: Template additions to MISGENDERED Hossain et al. [2023]\n",
       " Case Template\n",
       " Nominativebefore {name} came out, {det} {nom} identified differently.\n",
       " {name} is an indian filmmaker known for directing a movie, however {det} {nom} also directs television.\n",
       " {name} is a musician and dancer and {det} {nom} currently lives somewhere nearby.\n",
       " Accusativeupon recognizing {name}, the fan asked {det} {acc} for an autograph.\n",
       " {name} owns the car so be sure to give the keys back to {det} {acc} afterwards.\n",
       " {name} needs new shoes so I can give my other shoes to {det} {acc}.\n",
       " Pronominal Possessive{name} was a participant in {det} {gen-indep} own musical.\n",
       " {name} is an author of children’s fantasy, best known for {det} {gen-indep} book that won several awards.\n",
       " {name} is a singer-songwriter-producer based in boston and {det} {gen-indep} debut album was released in march.\n",
       " Predicative Possessive{name} was working in this area, so the phone is probably {det} {gen-dep}.\n",
       " the story of {name}’s resilience and triumph over adversity is {det} {gen-dep} to own.\n",
       " name} said to me that the larger slice of pizza was mine, and the smaller one was {det} {gen-dep}.\n",
       " Reflexive{name} will read the book by {det} {reflex}.\n",
       " {name} needs to be by {det} {reflex} sometimes.\n",
       " {name} often works alone by {det} {reflex}.\n",
       " A.6 Ablations\n",
       " Table 7 provides results across all data splits for the 70M model. Table 8 provides results across model\n",
       " sizes for the 10% data resource ablation, so as to best mimic real-world low-resource circumstances.\n",
       " 15,metadata = {'page': 15, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 4010952d-b10c-4211-89be-d8798dd0dda8, next_node = 1a3e2027-4572-43d7-aea0-9434e94c32a6, parent_node = None, child_node = []),\n",
       " TextNode(id = 1a3e2027-4572-43d7-aea0-9434e94c32a6,text = Table 7: 70M Model Results Across Data Splits\n",
       " (a) Data Split= 10\n",
       " ModelPronoun Consistency ( ↑) Case Agreement ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.968 0.716 0.007 0.677 0.607 0.200 0.238 0.169 0.850\n",
       " TOrig+MFull 0.896 0.861 0.145 0.685 0.597 0.200 0.239 0.168 0.895\n",
       " TOrig+MLex 0.865 0.729 0.168 0.617 0.572 0.200 0.290 0.232 0.654\n",
       " TPTP+MFull 0.948 0.835 0.378 0.695 0.611 0.294 0.278 0.210 0.270\n",
       " TPTP-B+MFull 0.962 0.807 0.244 0.689 0.612 0.231 0.283 0.206 0.259\n",
       " TPTP+MLex 0.850 0.722 0.536 0.632 0.591 0.325 0.258 0.218 0.348\n",
       " TPTP-B+MLex 0.833 0.743 0.430 0.654 0.596 0.332 0.243 0.202 0.321\n",
       " (b) Data Split= 20\n",
       " ModelConsistency ( ↑) Case ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.968 0.715 0.007 0.677 0.716 0.200 0.238 0.169 0.850\n",
       " TOrig+MFull 0.932 0.814 0.137 0.666 0.853 0.200 0.271 0.180 0.873\n",
       " TOrig+MLex 0.861 0.733 0.181 0.638 0.573 0.200 0.276 0.199 0.675\n",
       " TPTP+MFull 0.965 0.886 0.359 0.719 0.597 0.364 0.253 0.192 0.338\n",
       " TPTP-B+MFull 0.953 0.873 0.185 0.701 0.598 0.325 0.269 0.195 0.340\n",
       " TPTP+MLex 0.822 0.709 0.480 0.605 0.586 0.322 0.301 0.237 0.349\n",
       " TPTP-B+MLex 0.832 0.701 0.324 0.639 0.583 0.328 0.301 0.226 0.340\n",
       " (c) Data Split= 30\n",
       " ModelConsistency ( ↑) Case ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.968 0.716 0.007 0.677 0.607 0.200 0.238 0.169 0.850\n",
       " TOrig+MFull 0.913 0.852 0.138 0.689 0.614 0.200 0.246 0.191 0.870\n",
       " TOrig+MLex 0.801 0.647 0.185 0.627 0.574 0.200 0.292 0.243 0.666\n",
       " TPTP+MFull 0.958 0.877 0.324 0.724 0.603 0.357 0.232 0.197 0.344\n",
       " TPTP-B+MFull 0.909 0.844 0.126 0.721 0.601 0.288 0.255 0.191 0.260\n",
       " TPTP+MLex 0.812 0.620 0.455 0.638 0.566 0.348 0.268 0.209 0.314\n",
       " TPTP-B+MLex 0.849 0.693 0.483 0.648 0.573 0.317 0.277 0.210 0.333\n",
       " (d) Data Split= 40\n",
       " ModelConsistency ( ↑) Case ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.968 0.716 0.007 0.677 0.607 0.200 0.238 0.169 0.850\n",
       " TOrig+MFull 0.924 0.793 0.099 0.670 0.607 0.200 0.254 0.196 0.855\n",
       " TOrig+MLex 0.822 0.652 0.182 0.637 0.564 0.200 0.311 0.226 0.682\n",
       " TPTP+MFull 0.960 0.864 0.268 0.669 0.588 0.321 0.251 0.200 0.339\n",
       " TPTP-B+MFull 0.967 0.862 0.117 0.677 0.594 0.307 0.234 0.203 0.339\n",
       " TPTP+MLex 0.853 0.615 0.484 0.644 0.572 0.331 0.290 0.216 0.332\n",
       " TPTP-B+MLex 0.849 0.620 0.414 0.636 0.573 0.337 0.289 0.229 0.331\n",
       " (e) Data Split= 50\n",
       " ModelConsistency ( ↑) Case ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.968 0.716 0.007 0.677 0.607 0.200 0.238 0.169 0.850\n",
       " TOrig+MFull 0.934 0.852 0.141 0.664 0.613 0.200 0.260 ,metadata = {'page': 16, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 2b17bb07-6839-49a1-81da-1590b3f0ecb1, next_node = 2e52d549-95a4-4543-9a2a-b0c8d2d8e597, parent_node = None, child_node = []),\n",
       " TextNode(id = 2e52d549-95a4-4543-9a2a-b0c8d2d8e597,text = 0.198 0.861\n",
       " TOrig+MLex 0.834 0.651 0.185 0.610 0.567 0.200 0.287 0.190 0.712\n",
       " TPTP+MFull 0.960 0.889 0.267 0.649 0.604 0.335 0.244 0.217 0.359\n",
       " TPTP-B+MFull 0.950 0.872 0.161 0.626 0.590 0.320 0.295 0.216 0.379\n",
       " TPTP+MLex 0.775 0.582 0.584 0.625 0.572 0.366 0.291 0.197 0.315\n",
       " TPTP-B +MLex 0.815 0.644 0.493 0.637 0.557 0.354 0.268 0.224 0.305\n",
       " 16,metadata = {'page': 16, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 1a3e2027-4572-43d7-aea0-9434e94c32a6, next_node = 9b1ad0bf-c78c-4e64-825a-babc28d55b98, parent_node = None, child_node = []),\n",
       " TextNode(id = 9b1ad0bf-c78c-4e64-825a-babc28d55b98,text = Table 8: Model Size Comparisons at Data Split=10\n",
       " (a) 160M Parameter Model Results\n",
       " ModelConsistency ( ↑) Case ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.799 0.765 0.000 0.745 0.644 0.199 0.087 0.065 0.954\n",
       " TOrig+MFull 0.789 0.615 0.156 0.716 0.609 0.201 0.194 0.203 0.692\n",
       " TOrig+MLex 0.773 0.701 0.200 0.703 0.593 0.200 0.202 0.169 0.783\n",
       " TPTP+MFull 0.802 0.649 0.309 0.740 0.606 0.289 0.220 0.182 0.147\n",
       " TPTP-B+MFull 0.791 0.658 0.097 0.667 0.599 0.244 0.209 0.210 0.253\n",
       " TPTP+MLex 0.785 0.608 0.518 0.729 0.599 0.398 0.191 0.148 0.314\n",
       " TPTP-B+MLex 0.812 0.605 0.536 0.734 0.602 0.336 0.175 0.164 0.251\n",
       " (b) 410m Parameter Model Results\n",
       " ModelConsistency ( ↑) Case ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.728 0.559 0.001 0.767 0.663 0.200 0.042 0.035 0.898\n",
       " TOrig+MFull 0.790 0.421 0.184 0.744 0.617 0.201 0.128 0.198 0.566\n",
       " TOrig+MLex 0.698 0.421 0.198 0.765 0.653 0.210 0.170 0.118 0.545\n",
       " TPTP+MFull 0.636 0.482 0.562 0.737 0.620 0.406 0.144 0.150 0.207\n",
       " TPTP-B+MFull 0.823 0.488 0.190 0.732 0.612 0.417 0.115 0.174 0.262\n",
       " TPTP+MLex 0.683 0.457 0.406 0.765 0.641 0.478 0.167 0.134 0.136\n",
       " TPTP-B+MLex 0.694 0.356 0.492 0.774 0.655 0.487 0.177 0.174 0.122\n",
       " (c) 1.4B Parameter Model Results\n",
       " ModelConsistency ( ↑) Case ( ↑) Error ( ↓)\n",
       " He She Xe He She Xe He She Xe\n",
       " TOrig+MBaseline 0.785 0.665 0.003 0.782 0.714 0.200 0.037 0.034 0.928\n",
       " TOrig+MFull 0.767 0.587 0.179 0.750 0.628 0.201 0.247 0.246 0.363\n",
       " TOrig+MLex 0.801 0.560 0.155 0.793 0.646 0.200 0.166 0.355 0.552\n",
       " TPTP+MFull 0.847 0.565 0.450 0.730 0.661 0.450 0.241 0.180 0.201\n",
       " TPTP-B+MFull 0.719 0.539 0.357 0.734 0.629 0.447 0.191 0.223 0.185\n",
       " TPTP+MLex 0.768 0.446 0.363 0.818 0.623 0.383 0.121 0.246 0.193\n",
       " TPTP-B+MLex 0.797 0.578 0.356 0.809 0.627 0.482 0.201 0.267 0.272\n",
       " 17,metadata = {'page': 17, '/Author': '', '/CreationDate': 'D:20231222012004Z', '/Creator': 'LaTeX with hyperref', '/Keywords': '', '/ModDate': 'D:20231222012004Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/Subject': '', '/Title': '', '/Trapped': '/False', 'category': 'FILE', 'node_height': 0, 'node_length': 1}, prev_node = 2e52d549-95a4-4543-9a2a-b0c8d2d8e597, next_node = None, parent_node = None, child_node = [])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "loader = PDFLoader('datasets/papers_pdf')\n",
    "documents = loader.get_documents()\n",
    "images = loader.get_images(path = loader.path)\n",
    "bridge = DocumentBridge(documents, context = context)\n",
    "nodes = bridge.nodes(chunk_size = 2500)\n",
    "nodes\n",
    "#For images save surrounding image context for context + gpt/blip interpretation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399ffa9-9d01-4ce6-b19e-2f4120e3eeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id = 8c5b989b-40e8-4a35-b80c-22a3912ea683, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 1, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = c639d307-43f2-419c-8a8d-82393c942d28, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 2, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = ce64e44a-5ee6-4da1-8325-4c93d80c958d, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 3, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = 12e9b34a-a1b5-4bc1-a2c7-8f64e8adb04f, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 4, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = c45715cb-2000-4609-bad9-29e5d96c9f03, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 5, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = 68404b28-2a4c-44ea-b05e-be411a8bf0a6, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 6, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = ad58f1da-e26b-474e-854c-9a67141c9271, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 7, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0),\n",
       " Document(id = 85b74066-8825-40bd-8b8b-d3f299803298, name = Symbolic Numeric Planning with Patterns, metadata = {'page': 8, '/CreationDate': 'D:20231218020935Z', '/Creator': 'TeX', '/ModDate': 'D:20231218020935Z', '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', '/Producer': 'pdfTeX-1.40.25', '/TemplateVersion': '2024.1', '/Trapped': '/False'}, n_nodes = 0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "store = DocumentStore(documents)\n",
    "ids = store.ids()\n",
    "store.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdd428-3ac7-4ba4-9284-b263f98054e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "# Maybe I can make it so the bridge knows if an object is a Node or a Doc when inputting it and serves for both. \n",
    "documents = store.get()\n",
    "bridge = DocumentBridge(documents, context = context)\n",
    "nodes = bridge.nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2411c4-df1a-4a5b-8259-6666f6ef15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f46c1a-060e-4a3a-b761-12f53c733bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfe06c-ab4c-45d2-a68c-2aa5cca8896b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4a378-177c-43b0-8b05-ed573ec0f410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3c177-be06-47fd-ac69-d2ce2c3efd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
