{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from nanorag.base import *\n",
    "from nanorag.store import *\n",
    "from nanorag.context import *\n",
    "from nanorag.llm import *\n",
    "from nanorag.loaders import *\n",
    "from typing import Union, List, Dict, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26dc57885a1482ab379ed0d88a2c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "context = ModelContext()\n",
    "context.set_default()\n",
    "store = DocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing I would start exploring by having a document I want to be able to retrieve information from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My naive implementation would be an index for the embedding and mapping with the node index. Lets try that.\n",
    "\n",
    "Todo: \n",
    "\n",
    "* Try out different scoring strategies weighting other type of things, like metadata similarity, options that have been picked by an LLM and such.\n",
    "* Docker container to rapidly deploy agents.\n",
    "* Support own models apart from sentence transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| Export\n",
    "#Won't be used for now but serves for dependency injection for the index, to try diff retrieval strategies and combine them.\n",
    "#Will separate the retrieval strategies in the future.  \n",
    "class Retriever(ABC):\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query_embedding, embeddings, top_k):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VectorNodesIndex: #Compatible with TextNode right now. Storage of reference of certain nodes.\n",
    "    #Try out with SVM retrieval strategy and other ones. \n",
    "    #Question. Treat docstore the\n",
    "    #TODO: Proper context validation\n",
    "    \"\"\"Inside here the embeddings stored are normalized. So when doing operations with vectors has to be kept into account. \"\"\"\n",
    "    def __init__(self, context): #May not be needed in postgres. \n",
    "        self.idx_to_node = {}\n",
    "        self.idx = np.array([], dtype=np.int64)\n",
    "        self.context = context\n",
    "        #This line below accepts huggingface embeddings format. \n",
    "        self.embedding_dim = self.context.embedding[1].word_embedding_dimension\n",
    "        if self.embedding_dim is not None:\n",
    "            self.embeddings = np.empty((0, self.embedding_dim))\n",
    "        else:\n",
    "            self.embeddings = np.array([])\n",
    "        self.retrieval_strategies = {\n",
    "            \"dot_product\": self.retrieve_dot_product,\n",
    "        }\n",
    "\n",
    "    def add(self, nodes: Union[TextNode, List[TextNode]]): #Embed with non excluded content. \n",
    "        ##TODO: Add nodes to docstore that is referenced there by default. \n",
    "        # And retrive them directly\n",
    "        if isinstance(nodes, TextNode):\n",
    "            nodes = [nodes]\n",
    "        elif isinstance(nodes, list):\n",
    "            new_embeddings = np.vstack([node.embedding for node in nodes])\n",
    "            if self.embeddings.size == 0:\n",
    "                self.embeddings = new_embeddings\n",
    "            else:\n",
    "                self.embeddings = np.append(self.embeddings, new_embeddings, axis=0)\n",
    "            node_idx = np.arange(len(self.idx), self.embeddings.shape[0])\n",
    "            for node, idx in zip(nodes, node_idx):\n",
    "                self.idx_to_node[idx] = node.id\n",
    "                node.idx_ref = idx\n",
    "            self.idx = np.concatenate((self.idx, node_idx))\n",
    "\n",
    "    def get_node_ids(self, idx_refs: List[int]):\n",
    "        \"\"\"Providing a list of idx_refs of embeddings of the nodes, get the corresponding node ids\"\"\"\n",
    "        return [self.idx_to_node[idx_ref] for idx_ref in idx_refs]\n",
    "\n",
    "    def get_embedding(self, idx_ref: Union[List[int], int]):\n",
    "        \"\"\"Providing the idx_ref of the node, or nodes get the embedding\"\"\"\n",
    "        if isinstance(idx_ref, np.int64) or isinstance(idx_ref, int):\n",
    "            idx_ref = np.array([idx_ref], dtype = np.issubdtype(type(idx_ref), np.integer))\n",
    "        if isinstance(idx_ref, list):\n",
    "            idx_ref = np.array(idx_ref, dtype= np.int64)\n",
    "        if isinstance(idx_ref, np.ndarray):\n",
    "            return self.embeddings[idx_ref]\n",
    "\n",
    "    def retrieve(self, query_str: List[str], top_k: int = 10, strategy: str = 'dot_product'):\n",
    "        query_embedding = self.context.embedding.encode(query_str)\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)  # Ensure normalization\n",
    "        retrieve_method = self.retrieval_strategies.get(strategy, self.retrieve_dot_product)\n",
    "        top_k_idx_matches, top_k_scores = retrieve_method(query_embedding, top_k)\n",
    "        nodes = self.get_node_ids(top_k_idx_matches)\n",
    "        return nodes, top_k_scores\n",
    "    \n",
    "    def retrieve_dot_product(self, query_embedding, top_k):\n",
    "        similarity = self.cosine_similarity(query_embedding, self.embeddings.T)[0]\n",
    "        top_matches_similarity = np.argsort(similarity)[::-1]\n",
    "        top_k_idxs = top_matches_similarity[:top_k]\n",
    "        top_k_scores = similarity[top_k_idxs]\n",
    "        return top_k_idxs, top_k_scores\n",
    "    \n",
    "    def cosine_similarity(self,vecA, vecB):\n",
    "        return np.dot(vecA, vecB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "index = VectorNodesIndex(context)\n",
    "loader = PDFLoader('datasets/papers_pdf', store = store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "documents = loader.get_documents()\n",
    "document = DocumentBridge(documents, context = context).to_doc()\n",
    "nodes = DocumentBridge(document, context = context).to_nodes()\n",
    "document.save()\n",
    "store.add(nodes)\n",
    "index.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "node_ids, top_k_scores = index.retrieve(['What does deep learning do for mathematics in the industry of artificial intelligence?'], top_k = 5)\n",
    "top_nodes = store.get(node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
